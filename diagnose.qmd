---
title: "Comparing Planning Algorithms"
format: 
  html:
    code-fold: true
    fig-width: 10 
---

```{r}
#| label: r-setup
#| include: false

library(JuliaCall)
library(tidyverse)
library(posterior)
library(tidybayes)

theme_set(theme_minimal())
```

```{julia}
#| label: julia-setup
#| include: false

import Pkg
Pkg.activate(".")

using FundingPOMDPs
using MCTS, POMDPs, D3Trees, ParticleFilters, Distributions
using DataFrames, DataFramesMeta
using Pipe, Serialization

import SplitApplyCombine

include("diag_util.jl")
```

```{julia}
#| label: params
#| include: false

sim_file_suffix = "_1000"
util_model = ExponentialUtilityModel(0.25)
discount = 0.95
accum_rewards = true 
maxstep = 15
use_ex_ante_reward = true 
nprograms = 10
actlist = @pipe SelectProgramSubsetActionSetFactory(nprograms, 1) |> FundingPOMDPs.actions(_).actions
```

```{r}
#| include: false

maxstep <- julia_eval("maxstep")
nprograms <- julia_eval("nprograms")
discount <- julia_eval("discount")
```

```{julia}
#| label: load-sim-data
#| output: false

evalsecond_sim_data = deserialize("temp-data/evalsecond_sim$(sim_file_suffix).jls")  
all_sim_data = @pipe deserialize("temp-data/sim$(sim_file_suffix).jls") |> 
  vcat(_, evalsecond_sim_data; cols = :union)
  
pftdpw_sim_data = @subset(all_sim_data, :plan_type .== "pftdpw")
random_sim_data = @subset(all_sim_data, :plan_type .== "random")
freq_sim_data = @subset(all_sim_data, :plan_type .== "freq")
evalsecond_sim_data = @subset(all_sim_data, :plan_type .== "evalsecond")
```

```{julia}
#| label: prepare-util-data
#| output: false

do_nothing_reward = @pipe @subset(all_sim_data, :plan_type .== "none") |> 
  get_do_nothing_plan_data(_, util_model) 

do_best_reward = @pipe @subset(all_sim_data, :plan_type .== "none") |>
    dropmissing(_, :state) |> 
    @select(
        _,
        :actual_reward = map(get_program_reward, :state),
        :actual_ex_ante_reward = map(s -> get_program_reward(s, eval_getter = dgp), :state),
        :plan_type = "best"
    )

util_data = @pipe all_sim_data |> 
    vcat(_, do_best_reward, do_nothing_reward, cols = :union) |> 
    @select!(_, :plan_type, :actual_reward, :actual_ex_ante_reward, :step = repeat([collect(1:maxstep)], length(:plan_type))) |> 
    groupby(_, :plan_type) |> 
    transform!(_, eachindex => :sim) |> 
    flatten(_, Not([:plan_type, :sim]))
```

```{r}
#| label: util-diff-data

util_data <- julia_eval("util_data")

vn_util_diff <- util_data |> 
  unnest(c(actual_reward, actual_ex_ante_reward)) |> 
  pivot_longer(!c(sim, plan_type, step), names_to = "reward_type", names_pattern = r"{actual_(.*)_reward}", values_to = "reward") |> 
  mutate(reward_type = coalesce(reward_type, "ex_post")) %>%  
  left_join(filter(., fct_match(plan_type, "no impl")) |> select(!plan_type), by = c("reward_type", "sim", "step"), suffix = c("", "_no_impl")) |> 
  filter(!fct_match(plan_type, "no impl")) |> 
  mutate(reward_diff = reward - reward_no_impl) |> 
  arrange(step) |> 
  group_by(plan_type, reward_type, sim) |> 
  mutate(
    discounted_reward_diff = (discount^(step - 1)) * reward_diff,
    accum_reward_diff = cumsum(reward_diff),
    discounted_accum_reward_diff = cumsum(discounted_reward_diff)
  ) |> 
  ungroup() |> 
  pivot_longer(c(reward_diff, discounted_reward_diff, accum_reward_diff, discounted_accum_reward_diff), values_to = "reward_diff") |> 
  mutate(
    accum = str_detect(name, fixed("accum")),
    discounted = str_detect(name, fixed("discounted"))
  ) |> 
  select(!name)
```

```{r, fig.width=10, fig.height=10}
vn_util_diff |>
  filter(accum, discounted, fct_match(plan_type, c("pftdpw", "freq", "random", "none", "evalsecond"))) |> 
  ggplot(aes(step)) +
  tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Mean"), .width = 0.0, linewidth = 0.25, point_interval = mean_qi) +
  # tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Median"), .width = 0.0, linewidth = 0.25, point_interval = median_qi) +
  scale_x_continuous("Step", breaks = seq(maxstep)) +
  scale_y_continuous("Accumulated Utility Difference", breaks = seq(0, 2, 0.1)) +
  #scale_linetype_manual("", values = c("Mean" = "dashed", "Median" = "solid")) +
  scale_color_discrete(
    "Planning Type", 
    labels = c("best" = "Best", "freq" = "Random (Frequentist)", "none" = "No Evaluation", "pftdpw" = "PFTDPW", random = "Random (Bayesian)", evalsecond = "Evaluate Second Best (Bayesian)"), 
    aesthetics = c("color", "fill")
  ) +
  facet_wrap(vars(reward_type), ncol = 1, scales = "free_y", labeller = as_labeller(c(ex_ante = "ex ante", ex_post = "ex post"))) +
  labs(title = "Accumulated Utility Improvement Compared to No Implementation") +
  theme(panel.grid.minor.x = element_blank()) +
  guides(linetype = "none") +
  NULL
```

```{r}
vn_util_diff |>
  filter(step == 1, accum, discounted, fct_match(plan_type, c("pftdpw", "freq", "evalsecond"))) |> 
  ggplot(aes(y = plan_type)) +
  tidybayes::stat_pointinterval(aes(x = reward_diff), point_interval = mean_qi, .width = 0.0) +
  geom_vline(xintercept = 0, linetype = "dotted") +
  scale_y_discrete(
    "Planning Type", 
    labels = c("best" = "Best", "freq" = "Frequentist", "none" = "No Evaluation", "pftdpw" = "PFTDPW", random = "Random")
  ) +
  scale_x_continuous("Accumulated Utility Difference") +
  labs(title = "Accumulated Utility Improvement Compared to No Implementation", subtitle = "First Step Only") +
  facet_wrap(vars(reward_type), ncol = 1, scales = "free_y", labeller = as_labeller(c(ex_ante = "ex ante", ex_post = "ex post"))) +
  NULL
```

```{r, fig.width=10}
vn_util_diff |> 
  filter(!fct_match(plan_type, "best"), discounted, accum) |>
  ggplot(aes(step, reward_diff)) +
  #geom_line(aes(group = str_c(sim, plan_type)), alpha = 0.05) +
  tidybayes::stat_lineribbon(aes(linetype = "Mean"), .width = 0.0, linewidth = 0.25, point_interval = mean_qi) +
  tidybayes::stat_lineribbon(aes(linetype = "Median"), .width = 0.5, alpha = 0.25, linewidth = 0.25, point_interval = median_qi) +
  facet_grid(rows = vars(reward_type), cols = vars(plan_type)) +
  theme(legend.position = "bottom")
```

```{r}
vn_util_diff |> 
  filter(step == maxstep, accum, fct_match(reward_type, "ex_ante")) |> 
  ggplot(aes(y = plan_type, x = reward_diff)) +
  geom_col(alpha = 0.5) +
  NULL
```

```{julia}
all_rewards = @pipe all_sim_data |> 
    @subset(_, :plan_type .== "none") |> 
    get_rewards_data.(_.state, Ref(actlist), Ref(util_model)) |>
    [@transform!(rd[2], :sim = rd[1]) for rd in enumerate(_)] |>
    vcat(_...) |>
    insertcols!(_, :reward_type => "actual")

obs_act = @pipe [pftdpw_sim_data, random_sim_data, freq_sim_data, evalsecond_sim_data] |> 
  [vcat(get_actions_data.(getindex(d, :, :action))..., source = :sim) for d in _] |> 
  vcat(_..., source = :algo => ["pftdpw", "random", "freq", "evalsecond"])
```

```{r}
all_rewards <- julia_eval("all_rewards") 
obs_act <- julia_eval("obs_act")

ex_ante_reward_data <- all_rewards |> 
  filter(step == maxstep) |> 
  select(!step) |> 
  group_by(sim) |> 
  mutate(
    ex_ante_best = ex_ante_reward >= max(ex_ante_reward),
    reward_rank = min_rank(ex_ante_reward) - 1
  ) |> 
  ungroup()  
```


```{r, fig.height=8}
#| eval: false

ex_ante_reward_data |>
  mutate(actprog = factor(actprog)) |> 
  ggplot(aes(ex_ante_reward, tidytext::reorder_within(actprog, ex_ante_reward, sim))) +
  geom_col(aes(fill = ex_ante_best), alpha = 0.5, position = "dodge", show.legend = FALSE) +
  tidytext::scale_y_reordered() +
  scale_fill_manual("", values = c(`TRUE` = "darkred", `FALSE` = "black")) +
  facet_wrap(vars(sim), scales = "free")
```

```{r, fig.width=10, fig.height=12}
#| fig-height: 10 
#| eval: false

obs_act |>
  filter(between(sim, 70, 80)) |> 
  pivot_longer(c(implement_programs, eval_programs), names_to = "action_type", names_pattern = r"{(.+)_programs}", values_to = "pid") |> 
  left_join(ex_ante_reward_data, by = c("sim", "pid" = "actprog")) |>
  #glimpse()
  ggplot(aes(step, reward_rank, color = action_type)) +
  #ggplot(aes(step, tidytext::reorder_within(pid, ex_ante_reward, sim), color = action_type)) +
  geom_step() +
  geom_point() +
  # geom_hline(
  #   aes(yintercept = actprog), 
  #   linewidth = 2, alpha = 0.25,
  #   data = \(d) { 
  #     semi_join(ex_ante_reward_data, d, by = "sim") |> 
  #       filter(ex_ante_best)
  #   }
  # ) +
  scale_x_continuous(breaks = seq(maxstep)) +
  scale_y_continuous(breaks = 0:nprograms, c(0, 10)) +
  #tidytext::scale_y_reordered() +
  facet_grid(cols = vars(algo), rows = vars(sim), scales = "free_y") +
  theme(panel.grid.minor = element_blank(), legend.position = "top", axis.text.y = element_blank())
```

```{julia}
#| eval: false

b = pftdpw_sim_data.belief[2][1] 

test_data = @pipe data(b.progbeliefs[4]) |> 
  DataFrame.(_) |> 
  vcat(_..., source = :study)
```

```{r}
#| eval: false

test_data <- julia_eval("test_data")

library(cmdstanr)
library(posterior)

sim_model <-cmdstan_model("../FundingPOMDPs.jl/stan/sim_model.stan")

test_stan_data <- lst(
  fit = TRUE, sim = FALSE, sim_forward = FALSE,
  n_control_sim = 0,
  n_treated_sim = 0,
  n_study = 1,
  study_size = 50,
  y_control = test_data |> filter(!t) |> pull(y),
  y_treated = test_data |> filter(t) |> pull(y),
 
  sigma_eta_inv_gamma_priors = TRUE, 
  mu_sd = 1,
  tau_mean = 0,
  tau_sd = 0.5,
  sigma_sd = 0,
  eta_sd = c(0, 0, 0),
  sigma_alpha = 18.5,
  sigma_beta = 30,
  eta_alpha = 26.4,
  eta_beta = 20
)

fit <- sim_model$sample(test_stan_data, parallel_chains = 4)

dr <- as_draws_rvars(fit)
```

