---
title: The Funder's Sequential Problem [Work in Progress]
author: 
  - name: Karim Naguib
    email: karimn2.0@gmail.com
format: 
  html:
    number-sections: true
    code-fold: true
    fig-width: 9 
    toc: true
    toc-location: left
execute: 
  echo: false
knitr:
  opts_chunk: 
    cache: true 
reference-location: margin
bibliography: /home/karim/Documents/library.bib
---

```{r}
#| label: r-setup
#| include: false

library(JuliaCall)
library(tidyverse)
library(posterior)
library(tidybayes)

theme_set(theme_minimal())
```

```{julia}
#| label: julia-setup
#| include: false

import Pkg
Pkg.activate(".")

using FundingPOMDPs
using MCTS, POMDPs, D3Trees, ParticleFilters, Distributions
using DataFrames, DataFramesMeta
using Pipe, Serialization

import SplitApplyCombine

include("diag_util.jl")
```

```{julia}
#| label: params
#| include: false

sim_file_suffix = "_1000"
util_model = ExponentialUtilityModel(0.25)
discount = 0.95
accum_rewards = true 
maxstep = 15
use_ex_ante_reward = true 
nprograms = 10
actlist = @pipe SelectProgramSubsetActionSetFactory(nprograms, 1) |> FundingPOMDPs.actions(_).actions
```

```{r}
#| include: false

maxstep <- julia_eval("maxstep")
nprograms <- julia_eval("nprograms")
discount <- julia_eval("discount")

plan_labels <- c("no impl" = "No Implementation", none = "No Evaluation", random = "Random (Bayesian)", freq = "Random (Frequentist)", evalsecond = "Evaluate Second Best (Bayesian)",
                 freq_evalsecond = "Evaluate Second Best (Frequentist)", pftdpw = "PFT-DPW", best = "Best")
```

```{julia}
#| label: load-sim-data
#| output: false

all_sim_data = deserialize("temp-data/sim$(sim_file_suffix).jls") 
  
pftdpw_sim_data = @subset(all_sim_data, :plan_type .== "pftdpw")
random_sim_data = @subset(all_sim_data, :plan_type .== "random")
freq_sim_data = @subset(all_sim_data, :plan_type .== "freq")
evalsecond_sim_data = @subset(all_sim_data, :plan_type .== "evalsecond")
```

# Introduction

The aim of this study is to examine the challenges involved in selecting which interventions or programs to fund, and to propose ways to address them. Two main challenges stand out. The first is the problem of uncertainty regarding the true effect of an intervention on the welfare of a population. The second challenge is uncertainty about the heterogeneity of this effect across different contexts, including time and geography. While we may have some data from a specific context to inform our decision-making, we cannot simply assume that the same inference can be applied to other contexts.

This is a simulation study in which I first construct a probabilistic model representing the environment in which the agent (funder) must make decisions on which interventions to fund and which to re-evaluate.

# The Environment

In this experiment I'm going to use as simple as possible of an environment while trying to stay faithful to the most pertinent aspects of the real-world environment. The agent is assumed to be confronted with the set $\mathcal{K}$ of programs and have to decide which program(s) to fund.[$\mathcal{K} = \{1,\ldots,K\}$. In this study, we have $K = 10$.]{.aside} They will have to repeatedly make revisit this decision over a number of _steps_. They are also able to select program(s) for evaluation: conducting randomized trials to evaluate their effectiveness.

A good place to start is to model the environment as a multi-armed bandit (MAB). Each program/intervention is represented by such a bandit with a stochastic causal model: at every step in the sequential environment a new *state* -- drawn from a *hyperstate* -- determines the outcomes of the population targeted.[This is a broad simplification. More accurately we would distinguish between _programs_ and _populations_; different programs can be effective in different populations and different populations could be simultaneously targeted by a program.]{.aside} 

The decision the agent must make at each step is which program to implement and which to evaluate. This is different from typical MAB environments where the agent must decide one arm to pull, which in our context would be implementing and evaluating the same program (we take up the question of what actions are available to the agent in @sec-problem).

For each program $k$, we model the data generating process for each individual's outcome at step $t$ as,

$$
\begin{align*}
  Y_{t}(z) &\sim \mathtt{Normal}(\mu_{k[i],t} + z\cdot \tau_{k[i],t}, \sigma_{k[i]}) \\
  \\
  \mu_{kt} &\sim \mathtt{Normal}(\mu_k, \eta^\mu_k) \\
  \tau_{kt} &\sim \mathtt{Normal(\tau_k, \eta^\tau_k)}
\end{align*}
$$ [For simplicity, $\sigma_k$ is homoskedastic and does not vary over time.]{.aside}
where $z$ is a binary variable indicating whether a program is implemented or not. We therefore denote the state of a program to be $\boldsymbol{\theta}_{kt} = (\mu_{kt}, \tau_{kt}, \sigma_k)$. 

On the other hand, the hyperstate for each program, $\boldsymbol{\theta}_k = (\mu_k, \tau_k, \sigma_k, \eta^\mu_k, \eta^\tau_k)$, is drawn from the prior 
$$
\begin{align*}
  \mu_k &\sim \mathtt{Normal}(0, \xi^\mu) \\
  \tau_k &\sim \mathtt{Normal}(0, \xi^\tau) \\
  \sigma_k &\sim \mathtt{Normal}^+(0, \xi^\sigma) \\
  \eta^\mu_k &\sim \mathtt{Normal}^+(0, \xi^{\eta^\mu}) \\
  \eta^\tau_k &\sim \mathtt{Normal}^+(0, \xi^{\eta^\tau}), \\
\end{align*}
$$ 
where $\boldsymbol{\xi} = (\xi^\mu, \xi^\tau, \xi^\sigma, \xi^{\eta^\mu}, \xi^{\eta^\tau})$ are the hyperparameters for the environment. Thus we are modeling the counterfactual outcomes of program implementation as a hierarchical Bayesian model. This means that while each program has a fixed average baseline outcome, $\mu_k$, and average treatment effect, $\tau_k$, at every step normally distributed shocks, with mean zero, alter the realized averages. [With some abuse of notation, I will write $\boldsymbol{\theta_{kt}\sim\theta_k}$ and $\boldsymbol{\theta_k\sim\boldsymbol{\xi}}$.]{.aside} 

In this simple environment this hierarchical structure represents the heterogeneity of program effectiveness over time, emphasizing the limitations of a single evaluation of a program at a particular point in time. I'm making the simplifying assumption that this variation is purely an oscillation without any trends. Furthermore, agents should also be concerned about variations in effectiveness when programs are implemented in different contexts, but we ignore that in this environment assuming the time variation captures the general problem of heterogeneity over time and context.[Context here refers to geography or populations. Meta-analyses are typically aimed at understanding the generalizability of evaluations between contexts.]{.aside}

```{julia}
#| label: states-example-data
#| include: false

ep1_states = @pipe [@transform!(DataFrame(s.programstates), :t = t) for (t,s) in enumerate(all_sim_data.state[1])] |> 
  vcat(_...) |> 
  select(_, Not(:progdgp))
```

```{r}
#| label: fig-states-example
#| fig-cap: "Population outcomes over time for 10 example programs. Ribbons represent the mean outcome $\\pm \\sigma_p$."
#| cap-location: margin

ep1_states <- julia_eval("ep1_states") |> 
  transmute(programid, t, outcome_control = μ, outcome_treated = outcome_control + τ, sd = σ) |> 
  pivot_longer(starts_with("outcome_"), names_to = "z", names_prefix = "outcome_", values_to = "outcome") 

ep1_states |> 
  filter(t <= 15) |>  
  ggplot(aes(t, outcome)) +
  geom_line(aes(color = z)) +
  geom_ribbon(aes(ymin = outcome - sd, ymax = outcome + sd, fill = z), alpha = 0.1) +
  scale_color_discrete("", labels = str_to_title, aesthetics = c("color", "fill")) +
  labs(title = "Program Outcomes", x = "", y = "Y") +
  facet_wrap(vars(programid), ncol = 5) +
  theme(legend.position = "top")
```

Just as the funder is not aware of the true state of the world -- the true counterfactual model of all programs' effectiveness -- our agent never observes the state or hyperstate of a program. Instead, they are able to evaluate a program by collecting experimental data and updating their beliefs. I assume that the agent has a single observation (data from an evaluation) for each program under consideration. This could be data from an earlier experiment or could represent the agent's prior beliefs.

```{r}
#| label: fig-utility-fig
#| fig-cap: "The exponential utility function. In this study, we have $\\alpha = 0.25$."
#| fig-cap-location: bottom
#| fig-width: 4
#| column: margin

utility <- function(c, alpha) 1 - exp(-alpha * c)
expected_utility <- function(mu, sd, alpha) 1 - exp(-alpha * mu + alpha^2 * sd^2 / 2)

crossing(a = seq(0, 0.5, 0.125/2), c = seq(-4, 4, 0.1)) |> 
  mutate(u = utility(c, a)) |> 
  ggplot(aes(c, u)) +
  geom_line(data = \(x) filter(x, a == 0.25)) +
  geom_line(aes(group = a), alpha = 0.1) +
  labs(x = "y", y = "U(y)") +
  coord_cartesian(ylim = c(-2, 0.5)) +
  NULL
``` 
Finally, in evaluating which program to implement the agent is assumed to be maximizing welfare measured using a _utility function_. The program outcomes mentioned above are in terms of an abstract quantity such as income. Introducing a utility function means that we want to introduce the possibility of risk aversion and diminishing marginal utility: that it might be more optimal to increase the utility of those with a lower baseline utility and to implement programs with lower uncertainty. The utility function used is the _exponential utility_ function
$$
U(y;\alpha) = 1 - e^{- \alpha y},
$$
where $\alpha$ represents the degree of risk aversion. Typically, we would want to also take into consideration _cost-effectiveness_, however, in this formulation I assume that all programs have a unit cost.

If there is uncertainty[We focus on statistical uncertainty, ignoring moral uncertainty.]{.aside} in the outcomes or there is variability in outcomes in the population, we would want to work with expected utility. For example, given some means and standard deviations of outcomes over time, $\mu_{kt} + z\cdot \tau_{kt}$ and $\sigma_k$, respectively, the expected utility would be as in @fig-state-util-example. Since the agent also never observes the true state, this uncertainty also would have reductive affect on expected utility (if using a risk averse utility function). 

```{r}
#| label: fig-state-util-example
#| fig-cap: "Population expected utility over time for 10 example programs. $E_{Y\\sim\\boldsymbol{\\theta}}[U(Y)] = 1 - e^{-\\alpha\\mu + \\alpha^2 \\sigma^2/2}$."
#| cap-location: margin

ep1_states |> 
  mutate(eu = expected_utility(outcome, sd, 0.25)) |> 
  filter(t <= 15) |>  
  ggplot(aes(t, eu)) +
  geom_line(aes(color = z)) +
  scale_color_discrete("", labels = str_to_title, aesthetics = c("color", "fill")) +
  labs(title = "Program Expected Utility", x = "", y = "E[U(Y)]") +
  facet_wrap(vars(programid), ncol = 5) +
  theme(legend.position = "top")
```

# The Problem {#sec-problem}

Now that I described the environment the funder finds themselves in, I will take up the problem they are trying to solve. As described, they are confronted with $K$ programs and they must make two decisions (take two actions)

1. Select one program to fund (i.e., to implement) or none.
2. Select one program to evaluate.

At every $t$, the agent must choose a _(implement, evaluate)_-tuple.

$$
a_{t} \in \mathcal{A} = \{(m,v): m \in \mathcal{K}\cup\{0\}, v \in \mathcal{K}\}.
$$[Let $a^{m}_{kt}$ and $a^v_{kt}$ indicate if $k$ is implemented or evaluated, respectively.]{.aside}

This presents a relatively simpler problem than is typical of a multi-armed bandit problem; there is no real trade-off to make here between choosing the optimal program to fund and gathering more information on which is the optimal program.[The _exploration-exploitation_ tradeoff.]{.aside} Nevertheless, we are confronted by an _evaluative_ problem such that we must choose how to gather information most effectively. Furthermore, while a typical multi-armed bandit problem is not viewed as _sequential_ in the sense that an action at any step does not change future states. However, we can reformulate our problem to use the agent's _beliefs_ about parameters of the programs' causal models [@Morales2020;@Kochenderfer2022]. 

In that case, the problem is now _Markov Decision Process_ (MDP). The agent needs a _policy_, $\pi(b)$, that selects what action to take given the belief $b_t(\boldsymbol{\theta})$ over the continuous space of possible states.[Let the states of all the programs be $\boldsymbol{\theta}_t = (\boldsymbol{\theta}_{kt})_{k\in\mathcal{K}}$.]{.aside} Putting this together we get the _state-value_ function
$$
\begin{align*}
V_\pi(b_t) &= \int_{\Theta,\mathcal{O}} \left[R(a_t, \boldsymbol{\theta}) + \gamma V_\pi(b_{t+1})\right]p(o\mid\boldsymbol{\theta}, a_t)b_t(\boldsymbol{\theta})\,\textrm{d}\boldsymbol{\theta}\textrm{d}o \\ \\
a_t &= \pi(b_t) \\
R(a, \boldsymbol{\theta}) &= E_{Y\sim\boldsymbol{\theta}}[U(Y(a))] = \sum_{k\in\mathcal{K}} E_{Y_k\sim\boldsymbol{\theta}_k}\left[U(Y_{k}(a^m_k))\right], 
\end{align*}
$$[In this simulation study we set the discount rate to $\gamma = 0.95$.]{.aside}

where $o \in \mathcal{O}$ is the data collected based on the evaluation action for a particular program, and using it we update $b_t$ to $b_{t+1}$.

So given the current belief $b_t$ and the policy $\pi$, the agent estimates both the immediate reward and future discounted rewards -- given an updated belief $b_{t+1}$ continguent on the data collected $o$ -- and so forth recursively. Based on this the accumulated returns would be 
$$
G_t = \sum_{s=t}^T \gamma^{1-s}E_{\boldsymbol{\theta}_s\sim b_s}[R(a_s, \boldsymbol{\theta}_s)],
$$
where $T$ is the terminal step.[In this study, we set $T = 15$.]{.aside}  

Unlike a typical MDP the agent does not receive the actual realized reward, but must estimate it conditional on beliefs; program implementers do not automatically receive a reliable signal on the observed and counterfactual rewards. This is an important aspect of the funder's problem: while in a MAB we would normally observed a reward for the selected arm, or some noisy version of it, in the funder's environment, we observe an estimate of rewards for the evaluated program only -- all other rewards are inferred.[Also different from a MAB, we receive a reward from every program, or rather the population it targets.]{.aside} 

# The Plans

Now, we take up the question of policies.

1. _No evaluation,_ in which we never evaluate any of the programs and only use our initial beliefs, $b_0$, to decide which program to implement.
2. _Random evaluation,_ in which, at every $t$, we randomly select one of the $K$ programs to be evaluated.
3. _Evaluate second-best,_ in which, at every $t$, we select the program that has the second highest estimated reward for evaluation.  
4. _Particle Filter Tree with Progressive Depth Widening (PFT-DPW),_ in which we use an offline Monte Carlo Tree Search (MCTS) policy variant to select the program to evaluate [@Sunberg2018].

For all the policies experimented with, we maintain some kind of belief of the expected utility of implementation counterfactuals. For all of them we use a hierarchical Bayesian posterior to represent our updated beliefs. For the PFT-DPW policy we use a _particle filter_ to efficiently manage these beliefs as we iteratively build a tree of action-observation-belief trajectories. 

For the random and second-best policies we also consider a simple Frequentist null hypothesis significance testing approach: we run a regression on all the observed data; test whether the treatment effect is statistically significant at the 10% level; and if so, the point estimate is assumed to be the true treatment effect, and it is assumed to be zero otherwise. Using Frequentist inference essentially ignores uncertainty, but uses the expected utility based on $\sigma$. This form of inference is meant to highlight the shortcomings of a binary decision theory based on a significance test, or any kind of threshold in lieu of quantifying uncertainty. This is of course a major simplification but helps make the argument more intuitive.[GiveWell in fact look at point estimates of cost-effectiveness and use a threshold of some multiple of the cost-effectiveness of GiveDirectly, a cash transfer program. They also use subjective adjustments to the point estimates to account for uncertainty.]{.aside}  

```{julia}
#| label: rewards-and-actions 
#| include: false

all_rewards = @pipe all_sim_data |> 
    @subset(_, :plan_type .== "none") |> 
    get_rewards_data.(_.state, Ref(actlist), Ref(util_model)) |>
    [@transform!(rd[2], :sim = rd[1]) for rd in enumerate(_)] |>
    vcat(_...) |>
    insertcols!(_, :reward_type => "actual")

obs_act = @pipe all_sim_data |> 
  @rsubset(_, :plan_type in ["pftdpw", "random", "freq", "evalsecond", "freq_evalsecond"]) |> 
  groupby(_, :plan_type) |> 
  combine(_, d -> vcat(get_actions_data.(d.action)..., source = :sim)) 
```

```{r}
#| label: ex-ante-reward
#| include: false

all_rewards <- julia_eval("all_rewards") 
obs_act <- julia_eval("obs_act") |> 
  mutate(plan_type = factor(plan_type, levels = names(plan_labels)))

ex_ante_reward_data <- all_rewards |> 
  filter(step == maxstep) |> 
  select(!step) |> 
  group_by(sim) |> 
  mutate(
    ex_ante_best = ex_ante_reward >= max(ex_ante_reward),
    reward_rank = min_rank(ex_ante_reward) - 1
  ) |> 
  ungroup()  
```

```{r}
#| label: fig-actions
#| fig-cap: "Evaluate and implement actions over $K = 15$ steps for five example episodes (rows). For each each episode, we observe how the different policies behave (columns). The plot has been arranged such that the y-axis is in ascending order of _ex ante_ optimality."
#| fig-cap-location: margin

obs_act |>
  filter(between(sim, 1, 5)) |> 
  pivot_longer(c(implement_programs, eval_programs), names_to = "action_type", names_pattern = r"{(.+)_programs}", values_to = "pid") |> 
  left_join(ex_ante_reward_data, by = c("sim", "pid" = "actprog")) |>
  ggplot(aes(step, reward_rank, color = action_type)) +
  geom_step(alpha = 0.5) +
  geom_point(size = 0.85) +
  scale_x_continuous("Step", breaks = seq(maxstep)) +
  scale_y_continuous("", breaks = 0:nprograms, c(0, 10)) +
  scale_color_discrete("Action Type", labels = c(eval = "Evaluation", implement = "Implementation")) +
  facet_grid(cols = vars(plan_type), rows = vars(sim), scales = "free_y", labeller = labeller(plan_type = plan_labels)) +
  theme(panel.grid.minor = element_blank(), axis.text = element_blank(), legend.position = "top", strip.text.y.right = element_blank(), strip.text.x.top = element_text(size = 7),
        axis.ticks = element_blank())
```


# Results

```{julia}
#| label: prepare-util-data
#| include: false

do_nothing_reward = @pipe @subset(all_sim_data, :plan_type .== "none") |> 
  get_do_nothing_plan_data(_, util_model) 

do_best_reward = @pipe @subset(all_sim_data, :plan_type .== "none") |>
    dropmissing(_, :state) |> 
    @select(
        _,
        :actual_reward = map(get_program_reward, :state),
        :actual_ex_ante_reward = map(s -> get_program_reward(s, eval_getter = dgp), :state),
        :plan_type = "best"
    )

util_data = @pipe all_sim_data |> 
    vcat(_, do_best_reward, do_nothing_reward, cols = :union) |> 
    @select!(_, :plan_type, :actual_reward, :actual_ex_ante_reward, :step = repeat([collect(1:maxstep)], length(:plan_type))) |> 
    groupby(_, :plan_type) |> 
    transform!(_, eachindex => :sim) |> 
    flatten(_, Not([:plan_type, :sim]))
```

```{r}
#| label: util-diff-data
#| include: false

util_data <- julia_eval("util_data") |> 
  mutate(plan_type = factor(plan_type, levels = names(plan_labels)))

vn_util_diff <- util_data |> 
  unnest(c(actual_reward, actual_ex_ante_reward)) |> 
  pivot_longer(!c(sim, plan_type, step), names_to = "reward_type", names_pattern = r"{actual_(.*)_reward}", values_to = "reward") |> 
  mutate(reward_type = coalesce(reward_type, "ex_post")) %>%  
  left_join(filter(., fct_match(plan_type, "no impl")) |> select(!plan_type), by = c("reward_type", "sim", "step"), suffix = c("", "_no_impl")) |> 
  filter(!fct_match(plan_type, "no impl")) |> 
  mutate(reward_diff = reward - reward_no_impl) |> 
  arrange(step) |> 
  group_by(plan_type, reward_type, sim) |> 
  mutate(
    discounted_reward_diff = (discount^(step - 1)) * reward_diff,
    accum_reward_diff = cumsum(reward_diff),
    discounted_accum_reward_diff = cumsum(discounted_reward_diff)
  ) |> 
  ungroup() |> 
  pivot_longer(c(reward_diff, discounted_reward_diff, accum_reward_diff, discounted_accum_reward_diff), values_to = "reward_diff") |> 
  mutate(
    accum = str_detect(name, fixed("accum")),
    discounted = str_detect(name, fixed("discounted"))
  ) |> 
  select(!name)
```

```{r, fig.width=10, fig.height=10}
vn_util_diff |>
  filter(accum, discounted, fct_match(plan_type, c("pftdpw", "freq", "random", "none", "evalsecond", "freq_evalsecond"))) |> 
  ggplot(aes(step)) +
  tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Mean"), .width = 0.0, linewidth = 0.25, point_interval = mean_qi) +
  # tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Median"), .width = 0.0, linewidth = 0.25, point_interval = median_qi) +
  scale_x_continuous("Step", breaks = seq(maxstep)) +
  scale_y_continuous("Accumulated Utility Difference", breaks = seq(0, 2, 0.1)) +
  #scale_linetype_manual("", values = c("Mean" = "dashed", "Median" = "solid")) +
  scale_color_discrete(
    "Planning Type", 
    labels = plan_labels, 
    aesthetics = c("color", "fill")
  ) +
  facet_wrap(vars(reward_type), ncol = 1, scales = "free_y", labeller = as_labeller(c(ex_ante = "ex ante", ex_post = "ex post"))) +
  labs(title = "Accumulated Utility Improvement Compared to No Implementation") +
  theme(panel.grid.minor.x = element_blank()) +
  guides(linetype = "none") +
  NULL
```

```{r}
#| label: step1-returns-fig
#| fig-cap: "Accumulated utility gain at $t = 1$, comparing the best possible program choice, the Frequentist policy, and the Bayesian policy."
#| fig-cap-location: bottom
#| fig-width: 4
#| fig-height: 2
#| column: margin 

vn_util_diff |>
  filter(step == 1, accum, discounted, fct_match(plan_type, c("best", "random", "freq")), fct_match(reward_type, "ex_post")) |> 
  ggplot(aes(y = plan_type)) +
  #tidybayes::stat_pointinterval(aes(x = reward_diff), point_interval = mean_qi, .width = 0.0) +
  tidybayes::stat_dist_halfeye(aes(x = reward_diff), alpha = 0.5, .width = c(0.5, 0.8)) +
  geom_vline(xintercept = 0, linetype = "dotted") +
  scale_y_discrete(
    "", 
    labels = c("best" = "Best", "freq" = "Frequentist", random = "Random")
  ) +
  scale_x_continuous("Accumulated Utility Gain") +
  #labs(title = "Accumulated Utility Improvement Compared to No Implementation", subtitle = "First Step Only") +
  #facet_wrap(vars(reward_type), ncol = 1, scales = "free_y", labeller = as_labeller(c(ex_ante = "ex ante", ex_post = "ex post"))) +
  coord_cartesian(xlim = c(-0.75, 0.75)) +
  NULL
```

```{r}
#| eval: false

vn_util_diff |> 
  filter(!fct_match(plan_type, "best"), discounted, accum) |>
  ggplot(aes(step, reward_diff)) +
  #geom_line(aes(group = str_c(sim, plan_type)), alpha = 0.05) +
  tidybayes::stat_lineribbon(aes(linetype = "Mean"), .width = 0.0, linewidth = 0.25, point_interval = mean_qi) +
  tidybayes::stat_lineribbon(aes(linetype = "Median"), .width = 0.5, alpha = 0.25, linewidth = 0.25, point_interval = median_qi) +
  facet_grid(rows = vars(reward_type), cols = vars(plan_type)) +
  theme(legend.position = "bottom")
```


```{r, fig.height=8}
#| eval: false

ex_ante_reward_data |>
  mutate(actprog = factor(actprog)) |> 
  ggplot(aes(ex_ante_reward, tidytext::reorder_within(actprog, ex_ante_reward, sim))) +
  geom_col(aes(fill = ex_ante_best), alpha = 0.5, position = "dodge", show.legend = FALSE) +
  tidytext::scale_y_reordered() +
  scale_fill_manual("", values = c(`TRUE` = "darkred", `FALSE` = "black")) +
  facet_wrap(vars(sim), scales = "free")
```


```{julia}
#| eval: false

b = pftdpw_sim_data.belief[2][1] 

test_data = @pipe data(b.progbeliefs[4]) |> 
  DataFrame.(_) |> 
  vcat(_..., source = :study)
```

```{r}
#| eval: false

test_data <- julia_eval("test_data")

library(cmdstanr)
library(posterior)

sim_model <-cmdstan_model("../FundingPOMDPs.jl/stan/sim_model.stan")

test_stan_data <- lst(
  fit = TRUE, sim = FALSE, sim_forward = FALSE,
  n_control_sim = 0,
  n_treated_sim = 0,
  n_study = 1,
  study_size = 50,
  y_control = test_data |> filter(!t) |> pull(y),
  y_treated = test_data |> filter(t) |> pull(y),
 
  sigma_eta_inv_gamma_priors = TRUE, 
  mu_sd = 1,
  tau_mean = 0,
  tau_sd = 0.5,
  sigma_sd = 0,
  eta_sd = c(0, 0, 0),
  sigma_alpha = 18.5,
  sigma_beta = 30,
  eta_alpha = 26.4,
  eta_beta = 20
)

fit <- sim_model$sample(test_stan_data, parallel_chains = 4)

dr <- as_draws_rvars(fit)
```

# Discussion

# Conclusion

# References { .unnumbered }