---
title: "Comparing Planning Algorithms"
format: 
  html:
    number-sections: true
    code-fold: true
    fig-width: 9 
execute: 
  echo: false
knitr:
  opts_chunk: 
    cache: true 
reference-location: margin
bibliography: /home/karim/Documents/library.bib
---

```{r}
#| label: r-setup
#| include: false

library(JuliaCall)
library(tidyverse)
library(posterior)
library(tidybayes)

theme_set(theme_minimal())
```

```{julia}
#| label: julia-setup
#| include: false

import Pkg
Pkg.activate(".")

using FundingPOMDPs
using MCTS, POMDPs, D3Trees, ParticleFilters, Distributions
using DataFrames, DataFramesMeta
using Pipe, Serialization

import SplitApplyCombine

include("diag_util.jl")
```

```{julia}
#| label: params
#| include: false

sim_file_suffix = "_1000"
util_model = ExponentialUtilityModel(0.25)
discount = 0.95
accum_rewards = true 
maxstep = 15
use_ex_ante_reward = true 
nprograms = 10
actlist = @pipe SelectProgramSubsetActionSetFactory(nprograms, 1) |> FundingPOMDPs.actions(_).actions
```

```{r}
#| include: false

maxstep <- julia_eval("maxstep")
nprograms <- julia_eval("nprograms")
discount <- julia_eval("discount")
```

```{julia}
#| label: load-sim-data
#| output: false

all_sim_data = deserialize("temp-data/sim$(sim_file_suffix).jls") 
  
pftdpw_sim_data = @subset(all_sim_data, :plan_type .== "pftdpw")
random_sim_data = @subset(all_sim_data, :plan_type .== "random")
freq_sim_data = @subset(all_sim_data, :plan_type .== "freq")
evalsecond_sim_data = @subset(all_sim_data, :plan_type .== "evalsecond")
```

# Introduction

The aim of this study is to examine the challenges involved in selecting which interventions or programs to fund, and to propose ways to address them. Two main challenges stand out. The first is the problem of uncertainty regarding the true effect of an intervention on the welfare of a population. The second challenge is uncertainty about the heterogeneity of this effect across different contexts, including time and geography. While we may have some data from a specific context to inform our decision-making, we cannot simply assume that the same inference can be applied to other contexts.

This is a simulation study in which I first construct a probabilistic model representing the environment in which the agent (funder) must make decisions on which interventions to fund and which to re-evaluate.

# The Environment

In this experiment I'm going to use as simple as possible of an environment while trying to stay faithful to the most pertinent aspects of the real-world environment. The agent is assumed to be confronted with the set $\mathcal{K}$ of programs and have to decide which program(s) to fund.[$\mathcal{K} = \{1,\ldots,K\}$. In this study, we have $K = 10$.]{.aside} They will have to repeatedly make revisit this decision over a number of _steps_. They are also able to select program(s) for evaluation: conducting randomized trials to evaluate their effectiveness.

A good place to start is to model the environment as a multi-armed bandit (MAB). Each program/intervention is represented by such a bandit with a stochastic causal model: at every step in the sequential environment a new *state* -- drawn from a *hyperstate* -- determines the outcomes of the population targeted.[This is a broad simplification. More accurately we would distinguish between _programs_ and _populations_; different programs can be effective in different populations and different populations could be simultaneously targeted by a program.]{.aside} 

The decision the agent must make at each step is which program to implement and which to evaluate. This is different from typical MAB environments where the agent must decide one arm to pull, which in our context would be implementing and evaluating the same program (we take up the question of what actions are available to the agent in @sec-problem).

For each program $k$, we model the data generating process for each individual's outcome at step $t$ as,

$$
\begin{align*}
  Y_{t}(z) &\sim \mathtt{Normal}(\mu_{k[i],t} + z\cdot \tau_{k[i],t}, \sigma_{k[i]}) \\
  \\
  \mu_{kt} &\sim \mathtt{Normal}(\mu_k, \eta^\mu_k) \\
  \tau_{kt} &\sim \mathtt{Normal(\tau_k, \eta^\tau_k)}
\end{align*}
$$ [For simplicity, $\sigma_k$ is homoskedastic and does not vary over time.]{.aside}
where $z$ is a binary variable indicating whether a program is implemented or not. We therefore denote the state of a program to be $\theta_{kt} = (\mu_{kt}, \tau_{kt}, \sigma_k)$. 

On the other hand, the hyperstate for each program, $\theta_k = (\mu_k, \tau_k, \sigma_k, \eta^\mu_k, \eta^\tau_k)$, is drawn from the prior 
$$
\begin{align*}
  \mu_k &\sim \mathtt{Normal}(0, \xi^\mu) \\
  \tau_k &\sim \mathtt{Normal}(0, \xi^\tau) \\
  \sigma_k &\sim \mathtt{Normal}^+(0, \xi^\sigma) \\
  \eta^\mu_k &\sim \mathtt{Normal}^+(0, \xi^{\eta^\mu}) \\
  \eta^\tau_k &\sim \mathtt{Normal}^+(0, \xi^{\eta^\tau}), \\
\end{align*}
$$ 
where $\boldsymbol{\xi} = (\xi^\mu, \xi^\tau, \xi^\sigma, \xi^{\eta^\mu}, \xi^{\eta^\tau})$ are the hyperparameters for the environment. Thus we are modeling the counterfactual outcomes of program implementation as a hierarchical Bayesian model.[With some abuse of notation, I will write $\theta_{kt}\sim\theta_k$ and $\theta_k\sim\boldsymbol{\xi}$.]{.aside} 

In this simple environment this hierarchical structure represents the heterogeneity of program effectiveness over time, emphasizing the limitations of a single evaluation of a program at a particular point in time. I'm making the simplifying assumption that this variation is purely an oscillation without any trends. Furthermore, agents should also be concerned about variations in effectiveness when programs are implemented in different contexts, but we ignore that in this environment assuming the time variation captures the general problem of heterogeneity over time and context.[Context here refers to geography or populations. Meta-analyses are typically aimed at understanding the generalizability of evaluations between contexts.]{.aside}

```{julia}
#| label: states-example-data
#| include: false

ep1_states = @pipe [@transform!(DataFrame(s.programstates), :t = t) for (t,s) in enumerate(all_sim_data.state[1])] |> 
  vcat(_...) |> 
  select(_, Not(:progdgp))
```

```{r}
#| label: fig-states-example
#| fig-cap: "Population outcomes over time for 10 example programs. Ribbons represent the mean outcome $\\pm \\sigma_p$."
#| fig-height: 6
#| cap-location: margin

ep1_states <- julia_eval("ep1_states") |> 
  transmute(programid, t, outcome_control = μ, outcome_treated = outcome_control + τ, sd = σ) |> 
  pivot_longer(starts_with("outcome_"), names_to = "z", names_prefix = "outcome_", values_to = "outcome") 

ep1_states |> 
  filter(t <= 15) |>  
  ggplot(aes(t, outcome)) +
  geom_line(aes(color = z)) +
  geom_ribbon(aes(ymin = outcome - sd, ymax = outcome + sd, fill = z), alpha = 0.1) +
  scale_color_discrete("", labels = str_to_title, aesthetics = c("color", "fill")) +
  labs(title = "Program Outcomes", x = "", y = "Y") +
  facet_wrap(vars(programid), ncol = 3) +
  theme(legend.position = "top")
```

Just as the funder is not aware of the true state of the world -- the true counterfactual model of all programs' effectiveness -- our agent never observes the state or hyperstate of a program. Instead, they are able to evaluate a program by collecting experimental data and updating their beliefs. I assume that the agent has a single observation (data from an evaluation) for each program under consideration. This could be data from an earlier experiment or could represent the agent's prior beliefs.

```{r}
#| label: fig-utility-fun
#| fig-cap: "The exponential utility function. In this study, we have $\\alpha = 0.25$."
#| fig-cap-location: bottom
#| fig-width: 4
#| column: margin

utility <- function(c, alpha) 1 - exp(-alpha * c)
expected_utility <- function(mu, sd, alpha) 1 - exp(-alpha * mu + alpha^2 * sd^2 / 2)

crossing(a = seq(0, 0.5, 0.125/2), c = seq(-4, 4, 0.1)) |> 
  mutate(u = utility(c, a)) |> 
  ggplot(aes(c, u)) +
  geom_line(data = \(x) filter(x, a == 0.25)) +
  geom_line(aes(group = a), alpha = 0.1) +
  labs(x = "y", y = "U(y)") +
  coord_cartesian(ylim = c(-2, 0.5)) +
  NULL
``` 
Finally, in evaluating which program to implement the agent is assumed to be maximizing welfare measured using a _utility function_. The program outcomes mentioned above are in terms of an abstract quantity such as income. Introducing a utility function means that we want to introduce the possibility of risk aversion and diminishing marginal utility: that it might be more optimal to increase the utility of those with a lower baseline utility and to implement programs with lower uncertainty. The utility function used is the _exponential utility_ function
$$
U(y;\alpha) = 1 - e^{- \alpha y},
$$
where $\alpha$ represents the degree of risk aversion. Typically, we would want to also take into consideration _cost-effectiveness_, however, in this formulation I assume that all programs have a unit cost.

If there is uncertainty in the outcomes or there is variability in outcomes in the population, we would want to work with expected utility. For example, given some means and standard deviations of outcomes over time, $\mu_{kt} + z\cdot \tau_{kt}$ and $\sigma_k$, respectively, the expected utility would be as in @fig-state-util-example. Since the agent also never observes the true state, this uncertainty also would have reductive affect on expected utility (if using a risk averse utility function). 

```{r}
#| label: fig-state-util-example
#| fig-cap: "Population expected utility over time for 10 example programs. $E_{Y\\sim\\theta}[U(Y)] = 1 - e^{-\\alpha\\mu + \\alpha^2 \\sigma^2/2}$."
#| fig-height: 6
#| cap-location: margin

ep1_states |> 
  mutate(eu = expected_utility(outcome, sd, 0.25)) |> 
  filter(t <= 15) |>  
  ggplot(aes(t, eu)) +
  geom_line(aes(color = z)) +
  scale_color_discrete("", labels = str_to_title, aesthetics = c("color", "fill")) +
  labs(title = "Program Expected Utility", x = "", y = "E[U(Y)]") +
  facet_wrap(vars(programid), ncol = 3) +
  theme(legend.position = "top")
```

# The Problem {#sec-problem}

Now that I described the environment the funder finds themselves in, I will take up the problem they are trying to solve. As described, they are confronted with $K$ programs and they must make two decisions (take two actions)

1. Select one program to fund (i.e., to implement) or none.
2. Select one program to evaluate.

At every $t$, the agent must choose a _(implement, evaluate)_-tuple.

$$
a_{t} \in \mathcal{A} = \{(m,v): m \in \mathcal{K}\cup\{0\}, v \in \mathcal{K}\}.
$$[Let $a^{m}_{kt}$ and $a^v_{kt}$ indicate if $k$ is implemented or evaluated, respectively.]{.aside}

This presents a relatively simpler problem than is typical of a multi-armed bandit problem; there is no real trade-off to make here between choosing the optimal program to fund and gathering more information on which is the optimal program.[The _exploration-exploitation_ tradeoff.]{.aside} Nevertheless, we are confronted by an _evaluative_ problem such that we must choose how to gather information most effectively. Furthermore, while a typical multi-armed bandit problem is not viewed as _sequential_ in the sense that an action at any step does not change future states. However, we can reformulate our problem to use the agent's _beliefs_ about parameters of the programs' causal models [@Morales2020;@Kochenderfer2022]. 

In that case, the problem is now _Markov Decision Process_ (MDP). The agent needs a _policy_, $\pi(b)$, that selects what action to take given the belief $b_t(\theta)$ over the continuous space of possible states.[Let the states of all the programs be $\theta_t = (\theta_{kt})_{k\in\mathcal{K}}$.]{.aside} Putting this together we get the _state-value_ function
$$
\begin{align*}
V_\pi(b_t) &= \int_{\Theta,\mathcal{O}} \left[R(a_t, \theta) + \gamma V_\pi(b_{t+1})\right]p(o\mid\theta, a_t)b_t(\theta)\,\textrm{d}\theta\textrm{d}o \\ \\
a_t &= \pi(b_t) \\
R(a, \theta) &= E_{Y\sim\theta}[U(Y(a))] = \sum_{k\in\mathcal{K}} E_{Y_k\sim\theta_k}\left[U(Y_{k}(a^m_k))\right], 
\end{align*}
$$[In this simulation study we set the discount rate to $\gamma = 0.95$.]{.aside}

where $o \in \mathcal{O}$ is the data collected based on the evaluation action for a particular program, and using it we update $b_t$ to $b_{t+1}$.

So given the current belief $b_t$ and the policy $\pi$, the agent estimates both the immediate reward and future discounted rewards -- given an updated belief $b_{t+1}$ continguent on the data collected $o$ -- and so forth recursively. Based on this the accumulated returns would be 
$$
G_t = \sum_{s=t}^T \gamma^{1-s}E_{\theta_s\sim b_s}[R(a_s, \theta_s)],
$$
where $T$ is the terminal step.[In this study, we set $T = 15$.]{.aside}  

Unlike a typical MDP the agent does not receive the actual realized reward, but must estimate it conditional on beliefs; program implementers do not automatically receive a reliable signal on the observed and counterfactual rewards. This is an important aspect of the funder's problem: while in a MAB we would normally observed a reward for the selected arm, or some noisy version of it, in the funder's environment, we observe an estimate of rewards for the evaluated program only -- all other rewards are inferred.[Also different from a MAB, we receive a reward from every program, or rather the population it targets.]{.aside} 

# The Plans

# Results

For this experiment, I am going to draw 10 programs, $p \in \{1,\ldots,10\}$ per episode.


```{julia}
#| label: prepare-util-data
#| output: false

do_nothing_reward = @pipe @subset(all_sim_data, :plan_type .== "none") |> 
  get_do_nothing_plan_data(_, util_model) 

do_best_reward = @pipe @subset(all_sim_data, :plan_type .== "none") |>
    dropmissing(_, :state) |> 
    @select(
        _,
        :actual_reward = map(get_program_reward, :state),
        :actual_ex_ante_reward = map(s -> get_program_reward(s, eval_getter = dgp), :state),
        :plan_type = "best"
    )

util_data = @pipe all_sim_data |> 
    vcat(_, do_best_reward, do_nothing_reward, cols = :union) |> 
    @select!(_, :plan_type, :actual_reward, :actual_ex_ante_reward, :step = repeat([collect(1:maxstep)], length(:plan_type))) |> 
    groupby(_, :plan_type) |> 
    transform!(_, eachindex => :sim) |> 
    flatten(_, Not([:plan_type, :sim]))
```

```{r}
#| label: util-diff-data

util_data <- julia_eval("util_data")

vn_util_diff <- util_data |> 
  unnest(c(actual_reward, actual_ex_ante_reward)) |> 
  pivot_longer(!c(sim, plan_type, step), names_to = "reward_type", names_pattern = r"{actual_(.*)_reward}", values_to = "reward") |> 
  mutate(reward_type = coalesce(reward_type, "ex_post")) %>%  
  left_join(filter(., fct_match(plan_type, "no impl")) |> select(!plan_type), by = c("reward_type", "sim", "step"), suffix = c("", "_no_impl")) |> 
  filter(!fct_match(plan_type, "no impl")) |> 
  mutate(reward_diff = reward - reward_no_impl) |> 
  arrange(step) |> 
  group_by(plan_type, reward_type, sim) |> 
  mutate(
    discounted_reward_diff = (discount^(step - 1)) * reward_diff,
    accum_reward_diff = cumsum(reward_diff),
    discounted_accum_reward_diff = cumsum(discounted_reward_diff)
  ) |> 
  ungroup() |> 
  pivot_longer(c(reward_diff, discounted_reward_diff, accum_reward_diff, discounted_accum_reward_diff), values_to = "reward_diff") |> 
  mutate(
    accum = str_detect(name, fixed("accum")),
    discounted = str_detect(name, fixed("discounted"))
  ) |> 
  select(!name)
```

```{r, fig.width=10, fig.height=10}
vn_util_diff |>
  filter(accum, discounted, fct_match(plan_type, c("pftdpw", "freq", "random", "none", "evalsecond", "freq_evalsecond"))) |> 
  ggplot(aes(step)) +
  tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Mean"), .width = 0.0, linewidth = 0.25, point_interval = mean_qi) +
  # tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Median"), .width = 0.0, linewidth = 0.25, point_interval = median_qi) +
  scale_x_continuous("Step", breaks = seq(maxstep)) +
  scale_y_continuous("Accumulated Utility Difference", breaks = seq(0, 2, 0.1)) +
  #scale_linetype_manual("", values = c("Mean" = "dashed", "Median" = "solid")) +
  scale_color_discrete(
    "Planning Type", 
    labels = c("best" = "Best", "freq" = "Random (Frequentist)", "none" = "No Evaluation", "pftdpw" = "PFTDPW", random = "Random (Bayesian)", evalsecond = "Evaluate Second Best (Bayesian)",
               freq_evalsecond = "Evaluate Second Best (Frequentist)"), 
    aesthetics = c("color", "fill")
  ) +
  facet_wrap(vars(reward_type), ncol = 1, scales = "free_y", labeller = as_labeller(c(ex_ante = "ex ante", ex_post = "ex post"))) +
  labs(title = "Accumulated Utility Improvement Compared to No Implementation") +
  theme(panel.grid.minor.x = element_blank()) +
  guides(linetype = "none") +
  NULL
```

```{r}
vn_util_diff |>
  filter(step == 1, accum, discounted, fct_match(plan_type, c("pftdpw", "freq", "evalsecond"))) |> 
  ggplot(aes(y = plan_type)) +
  tidybayes::stat_pointinterval(aes(x = reward_diff), point_interval = mean_qi, .width = 0.0) +
  geom_vline(xintercept = 0, linetype = "dotted") +
  scale_y_discrete(
    "Planning Type", 
    labels = c("best" = "Best", "freq" = "Frequentist", "none" = "No Evaluation", "pftdpw" = "PFTDPW", random = "Random")
  ) +
  scale_x_continuous("Accumulated Utility Difference") +
  labs(title = "Accumulated Utility Improvement Compared to No Implementation", subtitle = "First Step Only") +
  facet_wrap(vars(reward_type), ncol = 1, scales = "free_y", labeller = as_labeller(c(ex_ante = "ex ante", ex_post = "ex post"))) +
  NULL
```

```{r, fig.width=10}
vn_util_diff |> 
  filter(!fct_match(plan_type, "best"), discounted, accum) |>
  ggplot(aes(step, reward_diff)) +
  #geom_line(aes(group = str_c(sim, plan_type)), alpha = 0.05) +
  tidybayes::stat_lineribbon(aes(linetype = "Mean"), .width = 0.0, linewidth = 0.25, point_interval = mean_qi) +
  tidybayes::stat_lineribbon(aes(linetype = "Median"), .width = 0.5, alpha = 0.25, linewidth = 0.25, point_interval = median_qi) +
  facet_grid(rows = vars(reward_type), cols = vars(plan_type)) +
  theme(legend.position = "bottom")
```

```{julia}
all_rewards = @pipe all_sim_data |> 
    @subset(_, :plan_type .== "none") |> 
    get_rewards_data.(_.state, Ref(actlist), Ref(util_model)) |>
    [@transform!(rd[2], :sim = rd[1]) for rd in enumerate(_)] |>
    vcat(_...) |>
    insertcols!(_, :reward_type => "actual")

obs_act = @pipe [pftdpw_sim_data, random_sim_data, freq_sim_data, evalsecond_sim_data] |> 
  [vcat(get_actions_data.(getindex(d, :, :action))..., source = :sim) for d in _] |> 
  vcat(_..., source = :algo => ["pftdpw", "random", "freq", "evalsecond"])
```

```{r}
all_rewards <- julia_eval("all_rewards") 
obs_act <- julia_eval("obs_act")

ex_ante_reward_data <- all_rewards |> 
  filter(step == maxstep) |> 
  select(!step) |> 
  group_by(sim) |> 
  mutate(
    ex_ante_best = ex_ante_reward >= max(ex_ante_reward),
    reward_rank = min_rank(ex_ante_reward) - 1
  ) |> 
  ungroup()  
```

```{r, fig.height=8}
#| eval: false

ex_ante_reward_data |>
  mutate(actprog = factor(actprog)) |> 
  ggplot(aes(ex_ante_reward, tidytext::reorder_within(actprog, ex_ante_reward, sim))) +
  geom_col(aes(fill = ex_ante_best), alpha = 0.5, position = "dodge", show.legend = FALSE) +
  tidytext::scale_y_reordered() +
  scale_fill_manual("", values = c(`TRUE` = "darkred", `FALSE` = "black")) +
  facet_wrap(vars(sim), scales = "free")
```

```{r, fig.width=10, fig.height=12}
#| fig-height: 10 
#| eval: false

obs_act |>
  filter(between(sim, 70, 80)) |> 
  pivot_longer(c(implement_programs, eval_programs), names_to = "action_type", names_pattern = r"{(.+)_programs}", values_to = "pid") |> 
  left_join(ex_ante_reward_data, by = c("sim", "pid" = "actprog")) |>
  #glimpse()
  ggplot(aes(step, reward_rank, color = action_type)) +
  #ggplot(aes(step, tidytext::reorder_within(pid, ex_ante_reward, sim), color = action_type)) +
  geom_step() +
  geom_point() +
  # geom_hline(
  #   aes(yintercept = actprog), 
  #   linewidth = 2, alpha = 0.25,
  #   data = \(d) { 
  #     semi_join(ex_ante_reward_data, d, by = "sim") |> 
  #       filter(ex_ante_best)
  #   }
  # ) +
  scale_x_continuous(breaks = seq(maxstep)) +
  scale_y_continuous(breaks = 0:nprograms, c(0, 10)) +
  #tidytext::scale_y_reordered() +
  facet_grid(cols = vars(algo), rows = vars(sim), scales = "free_y") +
  theme(panel.grid.minor = element_blank(), legend.position = "top", axis.text.y = element_blank())
```

```{julia}
#| eval: false

b = pftdpw_sim_data.belief[2][1] 

test_data = @pipe data(b.progbeliefs[4]) |> 
  DataFrame.(_) |> 
  vcat(_..., source = :study)
```

```{r}
#| eval: false

test_data <- julia_eval("test_data")

library(cmdstanr)
library(posterior)

sim_model <-cmdstan_model("../FundingPOMDPs.jl/stan/sim_model.stan")

test_stan_data <- lst(
  fit = TRUE, sim = FALSE, sim_forward = FALSE,
  n_control_sim = 0,
  n_treated_sim = 0,
  n_study = 1,
  study_size = 50,
  y_control = test_data |> filter(!t) |> pull(y),
  y_treated = test_data |> filter(t) |> pull(y),
 
  sigma_eta_inv_gamma_priors = TRUE, 
  mu_sd = 1,
  tau_mean = 0,
  tau_sd = 0.5,
  sigma_sd = 0,
  eta_sd = c(0, 0, 0),
  sigma_alpha = 18.5,
  sigma_beta = 30,
  eta_alpha = 26.4,
  eta_beta = 20
)

fit <- sim_model$sample(test_stan_data, parallel_chains = 4)

dr <- as_draws_rvars(fit)
```

# Discussion

# Conclusion

### References {-}