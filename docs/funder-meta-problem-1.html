<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.340">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Karim Naguib">
<meta name="dcterms.date" content="2023-04-08">

<title>Karim Naguib - The Funder’s Meta-Problem</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-SQNTJM494V"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-SQNTJM494V', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating slimcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#the-environment" id="toc-the-environment" class="nav-link" data-scroll-target="#the-environment"><span class="header-section-number">2</span> The Environment</a></li>
  <li><a href="#sec-problem" id="toc-sec-problem" class="nav-link" data-scroll-target="#sec-problem"><span class="header-section-number">3</span> The Problem</a></li>
  <li><a href="#the-plans" id="toc-the-plans" class="nav-link" data-scroll-target="#the-plans"><span class="header-section-number">4</span> The Plans</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">5</span> Results</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6</span> Conclusion</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="funder-meta-problem-1.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">The Funder’s Meta-Problem</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Karim Naguib </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 8, 2023</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    This study utilizes a simulation model to examine the impact of planning policies over time for an Effective Altruism funder focused on maximizing welfare through intervention selection. The results reveal a significant disparity in accumulated welfare between naive policies, such as relying on a single study to form beliefs about effectiveness, and more advanced probabilistic policies that optimize re-evaluation timing. This gap is more pronounced in sequential decision-making scenarios. Despite considering multiple factors and relying on a simplified model of the funder’s environment, the disparity between the best-performing policy and the hypothetical optimal policy remains substantial, indicating potential areas for improvement.
  </div>
</div>

</header>

<section id="introduction" class="level1 page-columns page-full" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The main goal of this simulation study is to analyze the sequential decision problem encountered by organizations involved in evaluating and funding charities from the perspective of Effective Altruism, which seeks to maximize the positive impact of donations on a global scale. In particular, the study aims to compare different decision-making policies for two key tasks:</p>
<ol type="i">
<li>Selecting programs to fund from a list of programs for which effectiveness is only partially observable, taking into account the inherent uncertainties in program outcomes and impact.</li>
<li>Determining which programs to re-evaluate in order to incrementally improve the decision-making process for program selection (i), by updating information and adjusting funding allocations accordingly.</li>
</ol>
<p>By investigating and evaluating various decision-making policies within this framework, the study aims to contribute insights into how organizations can make more informed and effective funding decisions, with the ultimate goal of maximizing positive impact and optimizing resource allocation for charitable purposes.</p>
<p>My objective is not to identify the optimal policy, but rather to explore the potential for welfare improvement using alternative policies to those conventionally used. It’s important to note that I am simplifying these policies for tractability and not considering all their complexities and context-specific adjustments that expert decision-makers may introduce. Nevertheless, I believe this study captures the essence of how conventionally used policies may underperform in certain scenarios.</p>
<p>Specifically, I aim to highlight the limitations of the following policies: (a) never re-evaluating programs and relying solely on initial evaluations, (b) randomly re-evaluating programs, and (c) using null hypothesis significance testing (NHST) in a simple heuristic policy. I will compare these conventional policies against policies that utilize a partially observable Markov decision process (POMDP) algorithm and a simple heuristic policy that uses Bayesian hierarchical models. Through my analysis, I have found that the alternative policies are able to increase accumulated discounted utility by at least 20 percent after a few steps.</p>
<p>Furthermore, it is important to highlight that while the framework of the implementation-evaluation problem in this study draws inspiration from the decision-making challenges faced by funding organizations in the realm of international development and global health charities, it is also relevant to the broader context of Effective Altruism. The decision problems faced by Effective Altruism practitioners often involve complex trade-offs and uncertainties, and the insights gained from this study may have broader implications for decision-making in these domains as well.</p>
<div class="page-columns page-full"><p>The funder’s problem is modeled as a sequence of decisions made at discrete intervals, given a finite set of programs with uncertain impact on a set of populations. The funder selects optimal programs to implement based on their beliefs about the counterfactual outcomes of these programs for their targeted populations, and decides what data to collect to update these beliefs for the next decision point. The environment and problem are intentionally kept simple to ensure tractability, with the understanding that further studies may revisit these assumptions iteratively.</p><div class="no-row-height column-margin column-container"><span class="">Focusing on epistemic uncertainty and ignoring moral uncertainty.</span></div></div>
<div class="page-columns page-full"><p>Thus the problem is modeled as a bandit problem, but without the restriction of only being able to evaluate implemented programs. Each program is assumed to target a particular population without any overlap, and the cost of implementation is held fixed and equal for all programs. There are no new programs entering the problem over time. The state of each program varies over time and is drawn from a hierarchical and stationary program hyperstate, which determines the data generating process for observed data when a program is evaluated.</p><div class="no-row-height column-margin column-container"><span class=""><em>State</em> here refers to the causal model determining outcome counterfactuals depending on whether a program is implemented or not. It is the data generating process from which we observe data when a program is evaluated.</span></div></div>
<p>While the optimal method to select a program for implementation is a probabilistic one, taking into account the distribution of counterfactual quantities and any available prior information, I also consider the commonly used null hypothesis significance testing (NHST) approach.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> However, my focus is not on comparing the probabilistic and NHST decision rules, but rather on the sequential nature of these decisions in the presence of heterogeneity in program effectiveness. I aim to examine the potential to improve welfare by enhancing the planning scheme used to select programs for re-evaluation, which I refer to as the <em>meta-problem</em>.</p>
<!-- Explain how the time hierarchy is similar to the context one. -->
</section>
<section id="the-environment" class="level1 page-columns page-full" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> The Environment</h1>
<div class="page-columns page-full"><p>As mentioned previously, in this study, a simplified environment is utilized while striving to capture the most relevant aspects of the real-world context. The funder is assumed to be faced with a set of programs, denoted as <span class="math inline">\(\mathcal{K}\)</span>, and must decide which program(s) to fund and which program(s) to re-evaluate. This decision needs to be made repeatedly over a series of steps.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\(\mathcal{K} = \{1,\ldots,K\}\)</span>. In this study, <span class="math inline">\(K = 10\)</span>.</span></div></div>
<div class="page-columns page-full"><p>The environment is modeled as a multi-armed bandit (MAB) framework, where each program or intervention is represented as a bandit with a stochastic causal model. In the sequential environment, at each step, a new <em>state</em> is drawn from a <em>hyperstate</em> that determines the outcomes of the targeted population. This hyperstate is used to simulate the underlying uncertainty and variability of real-world interventions, capturing the inherent uncertainties in program outcomes and their effects on the population.</p><div class="no-row-height column-margin column-container"><span class="">This is a broad simplification. In reality, we would distinguish between <em>programs</em> and <em>populations</em>; different programs can be effective in different populations and a program could simultaneously target different populations.</span></div></div>
<p>By employing a MAB framework and incorporating hyperstates, this study aims to capture the dynamic nature of decision-making in funding programs, where the funder must adapt and update their choices over time based on changing states and outcomes. This approach allows for exploring different decision-making policies and their impact on program selection and re-evaluation, in order to optimize resource allocation and improve the effectiveness of charitable funding decisions.</p>
<p>For each program <span class="math inline">\(k\)</span>, we model the data generating process for each individual’s outcome at step <span class="math inline">\(t\)</span> as,</p>
<div class="page-columns page-full"><p><span class="math display">\[\begin{align*}
  Y_{t}(z) &amp;\sim \mathtt{Normal}(\mu_{k[i],t} + z\cdot \tau_{k[i],t}, \sigma_{k[i]}) \\
  \\
  \mu_{kt} &amp;\sim \mathtt{Normal}(\mu_k, \eta^\mu_k) \\
  \tau_{kt} &amp;\sim \mathtt{Normal}(\tau_k, \eta^\tau_k)
\end{align*}\]</span>  where <span class="math inline">\(z\)</span> is a binary variable indicating whether a program is implemented or not, which means <span class="math inline">\(\tau_{kt}\)</span> is the average treatment effect. We therefore denote the state of a program to be <span class="math inline">\(\boldsymbol{\theta}_{kt} = (\mu_{kt}, \tau_{kt}, \sigma_k)\)</span>.</p><div class="no-row-height column-margin column-container"><span class="">For simplicity, <span class="math inline">\(\sigma_k\)</span> is homoskedastic and does not vary over time.</span></div></div>
<div class="page-columns page-full"><p>On the other hand, the hyperstate for each program, <span class="math inline">\(\boldsymbol{\theta}_k = (\mu_k, \tau_k, \sigma_k, \eta^\mu_k, \eta^\tau_k)\)</span>, is drawn from the prior <span class="math display">\[\begin{align*}
  \mu_k &amp;\sim \mathtt{Normal}(0, \xi^\mu) \\
  \tau_k &amp;\sim \mathtt{Normal}(0, \xi^\tau) \\
  \sigma_k &amp;\sim \mathtt{Normal}^+(0, \xi^\sigma) \\
  \eta^\mu_k &amp;\sim \mathtt{Normal}^+(0, \xi^{\eta^\mu}) \\
  \eta^\tau_k &amp;\sim \mathtt{Normal}^+(0, \xi^{\eta^\tau}), \\
\end{align*}\]</span> where <span class="math inline">\(\boldsymbol{\xi} = (\xi^\mu, \xi^\tau, \xi^\sigma, \xi^{\eta^\mu}, \xi^{\eta^\tau})\)</span> are the hyperparameters of the environment. This means that while each program has a fixed average baseline outcome, <span class="math inline">\(\mu_k\)</span>, and average treatment effect, <span class="math inline">\(\tau_k\)</span>, at every step, normally distributed shocks alter the realized averages. </p><div class="no-row-height column-margin column-container"><span class="">With some abuse of notation, I will write <span class="math inline">\(\boldsymbol{\theta_{kt}\sim\theta_k}\)</span> and <span class="math inline">\(\boldsymbol{\theta_k\sim\boldsymbol{\xi}}\)</span>.</span></div></div>
<div class="page-columns page-full"><p>In this environment, the hierarchical structure of the hyperstate represents the inherent heterogeneity of program effectiveness over time, highlighting the limitations of relying solely on a single evaluation of a program at a particular point in time. The assumption is made that this variation in effectiveness follows a purely oscillatory pattern without any trends. While funders should also be concerned about variations in effectiveness when programs are implemented in different contexts, this aspect is ignored in this simplified environment, assuming that the time variation captures the general problem of heterogeneity over time and context. As the states in the hyperstate vary randomly and independently, the objective of the funder is to learn about the underlying hyperstate, rather than predicting the next realized state.</p><div class="no-row-height column-margin column-container"><span class="">Context here refers to geography or populations. Meta-analyses are typically aimed at understanding the generalizability of evaluations between contexts.</span><span class="">Future iterations of this model could introduce some correlation between states over time.</span></div></div>
<div class="cell page-columns page-full" data-hash="funder-meta-problem-1_cache/html/fig-states-example_2f62a2e5079baaf9afabf3d2ffee806f">
<div class="cell-output-display page-columns page-full">
<div id="fig-states-example" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="funder-meta-problem-1_files/figure-html/fig-states-example-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;1: Population outcomes over time for 10 example programs. Ribbons represent the mean outcome <span class="math inline">\(\pm \sigma_p\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>The funder is never aware of the true state of the world — the true counterfactual model of all programs’ effectiveness — but they are able to evaluate a program by collecting data and updating their beliefs. I assume that the funder has an initial observation for each program under consideration. This could be data from an earlier experiment or could represent the funder’s or other experts’ prior beliefs.</p>
<div class="cell fig-cap-location-bottom page-columns page-full" data-hash="funder-meta-problem-1_cache/html/fig-utility-fig_9f7e872d79047dba64dd8702ecbb7a7f">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div id="fig-utility-fig" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="funder-meta-problem-1_files/figure-html/fig-utility-fig-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption class="figure-caption">Figure&nbsp;2: The exponential utility function, <span class="math inline">\(U(y;\alpha) = 1 - e^{- \alpha y},\)</span> where <span class="math inline">\(\alpha\)</span> represents the degree of risk aversion. In this study, we have <span class="math inline">\(\alpha = 0.25\)</span>.</figcaption>
</figure>
</div>
</div></div></div>
<p>In the evaluation of which program to implement, the agent is assumed to be maximizing welfare, which is measured using a utility function. The program outcomes, as mentioned earlier, are represented in terms of an abstract quantity, such as income. By incorporating a utility function, the analysis takes into account the possibility of risk aversion and diminishing marginal utility, recognizing that it may be more optimal to prioritize increasing the utility of individuals with lower baseline utility, even if it has relatively lower cost-effectiveness, or to choose programs with lower uncertainty. The utility function used in this study is the <em>exponential utility function</em>.</p>
<p>When there is uncertainty or variability in the outcomes of different programs, it is important to work with expected utilities to account for this variability. For instance, if we have information on the means and standard deviations of outcomes over time, denoted as <span class="math inline">\(\mu_{kt} + z\cdot \tau_{kt}\)</span> and <span class="math inline">\(\sigma_k\)</span>, respectively, the expected utility can be calculated as in <a href="#fig-state-util-example">Figure&nbsp;3</a>.</p>
<div class="cell page-columns page-full" data-hash="funder-meta-problem-1_cache/html/fig-state-util-example_e7f1d6186e38ef34f706c6c5369f9b0b">
<div class="cell-output-display page-columns page-full">
<div id="fig-state-util-example" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="funder-meta-problem-1_files/figure-html/fig-state-util-example-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;3: Population expected utility over time for 10 example programs. <span class="math inline">\(E_{Y_{kt}\sim\boldsymbol{\theta_{kt}}}[U(Y_{kt}(z))] = 1 - e^{-\alpha(\mu_{kt} + z\cdot\tau_{kt}) + \alpha^2 \sigma_k^2/2}\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-problem" class="level1 page-columns page-full" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> The Problem</h1>
<p>Now that the environment in which the funder operates has been described, the problem they are trying to solve can be addressed. The funder is confronted with a set of <span class="math inline">\(K\)</span> programs and must make two decisions, taking two actions:</p>
<ol type="i">
<li>Select one program to fund (i.e., to implement) or none.</li>
<li>Select one program to evaluate or none.</li>
</ol>
<p>At every time step <span class="math inline">\(t\)</span>, the agent must choose a tuple <span class="math inline">\((m,v)\)</span> from the action set <span class="math display">\[\mathcal{A} = \{(m,v): m, v\in \mathcal{K}\cup\{0\}\},\]</span> where <span class="math inline">\(m\)</span> represents the program to be funded (with <span class="math inline">\(0\)</span> representing no program), and <span class="math inline">\(v\)</span> represents the program to be evaluated (with <span class="math inline">\(0\)</span> representing no evaluation).</p>
<p>This presents a simpler problem than is typical of a multi-armed bandit problem; there is no real trade-off to make here between choosing the optimal program to fund and gathering more information on which is the optimal program. Nevertheless, we are confronted by an <em>evaluative</em> problem such that we must choose how to gather information most effectively. Furthermore, while a typical multi-armed bandit problem is not viewed as <em>sequential</em> in the sense that an action at any step does not change future states, we can reformulate our problem to use the funder’s <em>beliefs</em> about parameters of the programs’ causal models as the state <span class="citation" data-cites="Morales2020 Kochenderfer2022">(<a href="#ref-Morales2020" role="doc-biblioref">Morales 2020</a>; <a href="#ref-Kochenderfer2022" role="doc-biblioref">Kochenderfer, Wheeler, and Wray 2022</a>)</span>.</p>
<div class="page-columns page-full"><p>In that case, the problem is now a <em>Markov decision process</em> (MDP). The agent needs a <em>policy</em>, <span class="math inline">\(\pi(b)\)</span>, that selects what action to take given the belief <span class="math inline">\(b_t(\boldsymbol{\theta})\)</span> over the continuous space of possible states. Putting this together we get the <em>state-value</em> function <span id="eq-problem"><span class="math display">\[
\begin{equation*}
\begin{aligned}
V_\pi(b_{t-1}) &amp;= \int_{\Theta,\mathcal{O}} \left[R(a_t, \boldsymbol{\theta}) + \gamma V_\pi(b_{t})\right]p(o\mid\boldsymbol{\theta}, a_t)b_{t-1}(\boldsymbol{\theta})\,\textrm{d}\boldsymbol{\theta}\textrm{d}o \\ \\
a_t &amp;= \pi(b_t) \\
R(a, \boldsymbol{\theta}) &amp;= E_{Y\sim\boldsymbol{\theta}}[U(Y(a))] = \sum_{k\in\mathcal{K}} E_{Y_k\sim\boldsymbol{\theta}_k}\left[U(Y_{k}(a^m_k))\right],
\end{aligned}
\end{equation*}
\tag{1}\]</span></span></p><div class="no-row-height column-margin column-container"><span class="">Let the states of all the programs be <span class="math inline">\(\boldsymbol{\theta}_t = (\boldsymbol{\theta}_{kt})_{k\in\mathcal{K}}\)</span>.</span><span class="">In this simulation study we set the discount rate to <span class="math inline">\(\gamma = 0.95\)</span>.</span></div></div>
<p>where <span class="math inline">\(o \in \mathcal{O}\)</span> is the data collected based on the evaluation action for a particular program, and using it we update <span class="math inline">\(b_{t-1}\)</span> to <span class="math inline">\(b_{t}\)</span>.</p>
<div class="page-columns page-full"><p>So given the current belief <span class="math inline">\(b_{t-1}\)</span> and the policy <span class="math inline">\(\pi\)</span>, the agent estimates both the immediate reward and future discounted rewards – given an updated belief <span class="math inline">\(b_{t}\)</span> continguent on the data collected <span class="math inline">\(o\)</span> – and so forth recursively. Based on this the accumulated returns would be <span class="math display">\[
G_{\pi,t:T} = \sum_{r=t}^T \gamma^{r-t}E_{\boldsymbol{\theta}_r\sim b_{r-1}}[R(\pi(b_{r-1}), \boldsymbol{\theta}_r)],
\]</span> where <span class="math inline">\(T\)</span> is the terminal step.</p><div class="no-row-height column-margin column-container"><span class="">In this study, I use <span class="math inline">\(T = 15\)</span>.</span></div></div>
<div class="page-columns page-full"><p>Unlike a typical MDP, the agent in this case does not observe the actual realized reward at each step, but must estimate it conditional on their beliefs. Program implementers do not automatically receive a reliable signal on the observed and counterfactual rewards. This is an important aspect of the funder’s problem: while in an MDP, we would normally observe a reward for the selected action, or some noisy version of it; in the funder’s environment, all rewards are inferred.</p><div class="no-row-height column-margin column-container"><span class="">Also different from a MDP: we receive utility from every program, or rather from the population it targets.</span></div></div>
</section>
<section id="the-plans" class="level1 page-columns page-full" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The Plans</h1>
<p>Now, let’s discuss the policies that will be evaluated as part of the funder’s meta-problem:</p>
<ol type="1">
<li><em>No evaluation</em>, where we never re-evaluate any of the programs and only use our initial beliefs, denoted as <span class="math inline">\(b_0\)</span>, to decide which program to implement.</li>
<li><em>Random evaluation</em>, where at every time step <span class="math inline">\(t\)</span>, we randomly select one of the <span class="math inline">\(K\)</span> programs to be evaluated. For example, this happens if studies are conducted by researchers in an unplanned manner.</li>
<li><em>Evaluate second-best</em>, where at every time step <span class="math inline">\(t\)</span>, we select the program that has the second highest estimated reward for evaluation.</li>
<li><em>Particle Filter Tree with Progressive Depth Widening (PFT-DPW)</em>, where we use an offline Monte Carlo Tree Search (MCTS) policy variant, to select the program to evaluate <span class="citation" data-cites="Sunberg2018">(<a href="#ref-Sunberg2018" role="doc-biblioref">Sunberg and Kochenderfer 2018</a>)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ol>
<p>For all the policies being experimented with, we maintain a belief about the expected utility of implementation counterfactuals. We use a hierarchical Bayesian posterior to represent our updated beliefs for all the policies. For the PFT-DPW policy, we use a particle filter to efficiently manage these beliefs as we iteratively build a tree of action-observation-belief trajectories.</p>
<div class="cell page-columns page-full" data-hash="funder-meta-problem-1_cache/html/fig-actions_53aab292b532da9b56a32a1ae556556f">
<div class="cell-output-display page-columns page-full">
<div id="fig-actions" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="funder-meta-problem-1_files/figure-html/fig-actions-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;4: Evaluate and implement actions over <span class="math inline">\(K = 15\)</span> steps for five example episodes (rows). For each episode, we observe how the different policies behave (columns). The plot has been arranged such that the y-axis is in ascending order of <em>ex ante</em> optimality.</figcaption>
</figure>
</div>
</div>
</div>
<p>For the random and evaluate-second-best policies, I also consider a simple frequentist NHST approach. This involves running a regression on all the observed data, testing whether the treatment effect is statistically significant at the 10 percent level, and assuming the point estimate to be the true treatment effect if it is statistically significant, and assuming it to be zero otherwise. It’s important to note that using frequentist inference in this context essentially ignores uncertainty, but we still use the expected utility based on <span class="math inline">\(\sigma\)</span>. This form of inference is intended to highlight the limitations of binary decision-making based solely on statistical significance tests or arbitrary thresholds, instead of quantifying uncertainty. Although this approach is a simplification, it helps keep the argument intuitive.</p>
<!-- [GiveWell in fact look at point estimates of cost-effectiveness and use a threshold of some multiple of the cost-effectiveness of GiveDirectly, a cash transfer program. They also use subjective adjustments to the point estimates to account for uncertainty.]{.aside}   -->
<p>The motivation for selecting these policies/algorithms/heuristics is not to determine an optimal one, but rather to compare and contrast commonly used approaches. Specifically, the frequentist no-evaluation and random policies are chosen as they closely resemble the practices often employed by funders and implementers in real-world scenarios.</p>
<p>So given this set of policies, <span class="math inline">\(\Pi\)</span>, the meta-problem that we want to solve is choosing the best policy, <span id="eq-meta-problem"><span class="math display">\[
\max_{\pi \in \Pi} W_T(\pi) = E_{\boldsymbol{\theta}\sim\boldsymbol{\xi}}\left\{ \sum_{t=1}^T\gamma^{t-1}E_{\boldsymbol{\theta}_t\sim\boldsymbol{\theta}}[R(\pi(b_t), \boldsymbol{\theta}_t)] \right\}.
\tag{2}\]</span></span> Notice how this differs from the funder’s problem in <a href="#eq-problem">Equation&nbsp;1</a>: here we assume we know the hyperstates and states which we draw from the prior, <span class="math inline">\(\boldsymbol{\xi}\)</span>, not from beliefs, <span class="math inline">\(b\)</span>.</p>
</section>
<section id="results" class="level1 page-columns page-full" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Results</h1>
<div class="page-columns page-full"><p>In this simulation experiment, we run a total of <span class="math inline">\(S = r n_episodes\)</span> episodes.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> For each episode, we draw <span class="math inline">\(K\)</span> hyperstates from the prior, denoted as <span class="math inline">\(\boldsymbol{\theta}_s\sim\boldsymbol{\xi}\)</span>, and then for each step within the episode, we draw states denoted as <span class="math inline">\(\boldsymbol{\theta}_{st}\sim\boldsymbol{\theta}_s\)</span>. Next, we apply each of our policies to this episode, making decisions on which programs to implement and which ones to evaluate in order to update beliefs <span class="math inline">\(b{st}\)</span>. This allows us to observe the trajectory of <span class="math inline">\((b_{s,0}, a_{s,1}, o_{s,1}, b_{s,1}, a_{s,2}, o_{s,2}, b_{s,2},\ldots)\)</span> for each policy, given the same states and hyperstates.</p><div class="no-row-height column-margin column-container"><span class="">We actually solve <a href="#eq-meta-problem">Equation&nbsp;2</a> as <span class="math display">\[
\max_{\pi \in \Pi} \widetilde{W}_T(\pi) = \frac{1}{S} \sum_{s=1}^S \sum_{t=1}^T\gamma^{t-1}R(\pi(b_{st}), \boldsymbol{\theta}_{st}).
\]</span></span></div></div>
<p>To assess the performance of the policies, we compare their mean accumulated discounted utility to the same quantity when none of the programs are implemented. In Figure <a href="#fig-returns-compare">Figure&nbsp;5</a>, we can observe how this difference evolves over the <span class="math inline">\(T\)</span> steps of all the simulation episodes. We can see that the PFT-DPW and Bayesian evaluate-second-best policies perform the best, with higher accumulated discounted utility compared to other policies. The frequentist policies and the random Bayesian policy show lower performance. The no-evaluation policy, where decisions are made based only on the initial belief <span class="math inline">\(b_0\)</span>, performs the worst among all the policies.</p>
<div class="cell page-columns page-full" data-hash="funder-meta-problem-1_cache/html/fig-returns-compare_717c4ebe7c60aac80c45a2ec7c7012b8">
<div class="cell-output-display page-columns page-full">
<div id="fig-returns-compare" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="funder-meta-problem-1_files/figure-html/fig-returns-compare-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;5: Mean accumulated discounted utility gains, compared to a no program implementation policy, <span class="math display">\[\widetilde{W}_T(\pi) - \widetilde{W}_T(\pi^\emptyset),\]</span> where <span class="math inline">\(\pi^\emptyset(b) = (0,0),\forall b.\)</span></figcaption>
</figure>
</div>
</div>
</div>
<div class="cell fig-cap-location-bottom page-columns page-full" data-hash="funder-meta-problem-1_cache/html/fig-returns-percent-compare_6de1d5a620c36d18a5e7c97ccf627f14">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div id="fig-returns-percent-compare" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="funder-meta-problem-1_files/figure-html/fig-returns-percent-compare-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Percentage increase in mean accumulated discounted utility gain, <span class="math inline">\(\frac{\widetilde{W}_T(\pi) - \widetilde{W}_T(\pi')}{\widetilde{W}_T(\pi')}.\)</span></figcaption>
</figure>
</div>
</div></div></div>
<p>To provide a clearer comparison, we calculate the percentage difference between the highest performing policies and two baseline policies: (i) the no-evaluation policy, and (ii) the random Bayesian policy (which is roughly on par with the random frequentist policy). When compared to the policy of never re-evaluating a program once it is selected for implementation, we observe that the Bayesian evaluate-second-best and the PFT-DPW offline policies show an average accumulated welfare that is more than 20 percent higher after four episode steps, and surpasses 30 percent after seven steps. In comparison to the frequentist policies (evaluate-second-best and random), the highest performing policies show around 20 percent improvement after five steps.</p>
</section>
<section id="conclusion" class="level1 page-columns page-full" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion</h1>
<div class="cell fig-cap-location-bottom page-columns page-full" data-hash="funder-meta-problem-1_cache/html/fig-step1-returns_dcb3a6c6cf034ee51a5b59ed548ad0d1">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div id="fig-step1-returns" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="funder-meta-problem-1_files/figure-html/fig-step1-returns-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption class="figure-caption">Figure&nbsp;7: The distribution of utility gains at <span class="math inline">\(t = 1\)</span>, comparing the hypothetical best policy, the frequentist policy, and the Bayesian policy.</figcaption>
</figure>
</div>
</div></div></div>
<p>In conclusion, through the construction of a simple simulacrum of the problem faced by an Effective Altruism funder and the consideration of planning policies over time, it is evident that there is a significant gap in accumulated welfare between the more naive versions of policies and the more probabilistic and sophisticated policies. This gap becomes more pronounced when we consider the sequential problem, as opposed to the one-shot problem. For instance, in <a href="#fig-step1-returns">Figure&nbsp;7</a>, we can see that there is little difference between a frequentist NHST policy and a probabilistic one in the first step. However, as more steps are taken, differences between the policies become apparent, underscoring the importance of considering the longer-term implications of planning policies.</p>
<div class="cell page-columns page-full" data-hash="funder-meta-problem-1_cache/html/fig-returns-compare-include-best_617e061cfacc6fed9490bb3e86d8210e">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div id="fig-returns-compare-include-best" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="funder-meta-problem-1_files/figure-html/fig-returns-compare-include-best-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption class="figure-caption">Figure&nbsp;8: Mean accumulated discounted utility gains, compared to a no program implementation policy.</figcaption>
</figure>
</div>
</div></div></div>
<p>Even considering these factors, the gap between our best performing policy and the best possible hypothetical policy remains substantial (as seen in <a href="#fig-returns-compare-include-best">Figure&nbsp;8</a>). This suggests that there is likely more room for improvement in order to approach an optimal approach<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>Here are some possibilities for enhancing this study to better reflect real-world environments and develop more effective policies (in no particular order):</p>
<ul>
<li>Introduce varying program costs to consider the question of cost-effectiveness, as the cost of implementing different programs can have a significant impact on decision-making.</li>
<li>Explore how the concept of leverage could influence policy decisions, as certain programs may have a greater ability to leverage resources and create broader impact.</li>
<li>Allow for different population sizes in the simulation, as population size can affect the scalability and impact of interventions.</li>
<li>Consider the potential for programs to target multiple populations with some correlation, and for populations to support multiple programs with potential complementarity and substitution effects, as this can reflect the complexity and interrelatedness of real-world scenarios.</li>
<li>Incorporate non-stationarity into the hyperstates, effectively adding correlation between steps and potentially improving predictions and the need for re-evaluation, as real-world environments are dynamic and evolve over time.</li>
<li>Account for potential diminishing treatment effects over time as the control outcome moves closer to the treatment level, as this can affect the long-term effectiveness of interventions.</li>
<li>Consider the quality of programs or population compliance and how it may vary over time, as program quality and population behavior can impact outcomes in real-world scenarios.</li>
<li>Explore differences in evaluations for implemented programs versus non-implemented ones, as this can introduce potential scale effects and reflect the challenges of transitioning from proof-of-concept studies to scaled programs.</li>
<li>Restrict the implementation action choices to prevent rapid changes between programs due to fixed costs, as it may not always be feasible to shut down and resume programs in quick succession. For example, disallow restarting a program once abandoned to reflect real-world constraints.</li>
<li>Allow for program entry and exit over time to capture the dynamic nature of program availability and effectiveness.</li>
<li>Analyze the sensitivity of the simulation to varying the environment’s hyperparameters, <span class="math inline">\(\boldsymbol{\xi}\)</span>, to better understand the robustness of the results to different parameter settings.</li>
<li>Consider offline policy calculation methods (e.g., deep reinforcement learning) to further optimize policy performance.</li>
</ul>
<!-- Moral uncertainty, ambguity, and moral weights -->
<p>These enhancements can help to make the simulation model more accurate and reflective of real-world complexities, and enable the development of more effective policies for decision-making in the context of Effective Altruism funding.</p>
<div style="page-break-after: always;"></div>


<!-- -->


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Haber2022" class="csl-entry" role="listitem">
Haber, Noah A. 2022. <span>“<span class="nocase">GiveWell’s Uncertainty Problem</span>.”</span> <a href="https://www.metacausal.com/givewells-uncertainty-problem/">https://www.metacausal.com/givewells-uncertainty-problem/</a>.
</div>
<div id="ref-Kochenderfer2022" class="csl-entry" role="listitem">
Kochenderfer, Mykel J., Tim A. Wheeler, and Kyle H. Wray. 2022. <em><span class="nocase">Algorithms for Decision Making</span></em>. <a href="https://algorithmsbook.com/files/dm.pdf">https://algorithmsbook.com/files/dm.pdf</a>.
</div>
<div id="ref-Morales2020" class="csl-entry" role="listitem">
Morales, Miguel. 2020. <em><span>Grokking Deep Reinforcement Learning</span></em>. Manning Publications Co.
</div>
<div id="ref-Sunberg2018" class="csl-entry" role="listitem">
Sunberg, Zachary N., and Mykel J. Kochenderfer. 2018. <span>“<span class="nocase">Online algorithms for POMDPs with continuous state, action, and observation spaces</span>.”</span> <em>Proceedings International Conference on Automated Planning and Scheduling, ICAPS</em> 2018-June: 259–63. <a href="https://doi.org/10.1609/icaps.v28i1.13882">https://doi.org/10.1609/icaps.v28i1.13882</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Given a risk-neutral utility function and very weakly informed priors, both these methods are often assumed to result in very similar decisions. However, a winning entry in <a href="http://givewell.com">GiveWell’s</a> <a href="https://blog.givewell.org/2022/12/15/change-our-mind-contest-winners/">Change Our Minds Context</a> by <span class="citation" data-cites="Haber2022">Haber (<a href="#ref-Haber2022" role="doc-biblioref">2022</a>)</span> showed that threshold-based method, like NHST, suffers from bias caused by the winner’s curse phenomenon. A big difference between what I am investigating here and Haber’s work is that I am looking beyond the one-shot accept/reject decision.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The PFT-DPW algorithm is a hybrid approach for solving partially observable Markov decision processes (POMDPs) that combines particle filtering and tree-based search. It represents belief states using a tree data structure and uses double progressive widening to selectively expand promising regions of the belief state space. Particle weights are used to represent the probabilities of different belief states, and these weights are updated through the particle filtering and tree expansion process. Actions are selected based on estimated belief state values, and the tree is pruned to keep it computationally efficient.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Why not more? Each simulated episode can be time-consuming, especially when using the PFT-DPW policy which involves 1,000 iterations at every step before selecting a program for evaluation. Even simpler policies, such as the random policies, take time when updating beliefs using a Bayesian model that fits all the observed data for a program at every evaluation.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>It should be mentioned that in this experiment I did not attempt simulations very varying values of the prior hyperparameters, <span class="math inline">\(\boldsymbol{\xi}\)</span>, or the PFT-DPW algorithm hyperparameters.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> The Funder's Meta-Problem </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">aliases:</span><span class="co"> </span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  - index.html</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Karim Naguib</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    email: karimn2.0@gmail.com</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 4/8/2023 </span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    number-sections: true</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-width: 8 </span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-location: left</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">  pdf:</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">    number-sections: true</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-width: 8 </span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span><span class="co"> </span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: false</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="an">knitr:</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">  opts_chunk: </span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">    cache: true </span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> This study utilizes a simulation model to examine the impact of planning policies over time for an Effective Altruism funder focused on maximizing welfare through intervention selection. The results reveal a significant disparity in accumulated welfare between naive policies, such as relying on a single study to form beliefs about effectiveness, and more advanced probabilistic policies that optimize re-evaluation timing. This gap is more pronounced in sequential decision-making scenarios. Despite considering multiple factors and relying on a simplified model of the funder's environment, the disparity between the best-performing policy and the hypothetical optimal policy remains substantial, indicating potential areas for improvement. </span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: r-setup</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(JuliaCall)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(posterior)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidybayes)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="in">```{julia}</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: julia-setup</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="fu">import</span> Pkg</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>Pkg.activate(<span class="ot">"</span><span class="st">.</span><span class="ot">"</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>using FundingPOMDPs</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>using MCTS, POMDPs, D3Trees, ParticleFilters, Distributions</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>using DataFrames, DataFramesMeta</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>using Pipe, Serialization</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="fu">import</span> SplitApplyCombine</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>include(<span class="ot">"</span><span class="st">diag_util.jl</span><span class="ot">"</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="in">```{julia}</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: params</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>sim_file_suffix = <span class="ot">"</span><span class="st">_1000</span><span class="ot">"</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>util_model = ExponentialUtilityModel(<span class="fl">0.25</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>discount = <span class="fl">0.95</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>accum_rewards = true </span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>maxstep = <span class="dv">15</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>use_ex_ante_reward = true </span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>nprograms = <span class="dv">10</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>actlist = <span class="dt">@pipe</span> SelectProgramSubsetActionSetFactory(nprograms, <span class="dv">1</span>) |&gt; FundingPOMDPs.actions(_).actions</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>maxstep <span class="ot">&lt;-</span> <span class="fu">julia_eval</span>(<span class="st">"maxstep"</span>)</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>nprograms <span class="ot">&lt;-</span> <span class="fu">julia_eval</span>(<span class="st">"nprograms"</span>)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>discount <span class="ot">&lt;-</span> <span class="fu">julia_eval</span>(<span class="st">"discount"</span>)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>plan_labels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"no impl"</span> <span class="ot">=</span> <span class="st">"No Implementation"</span>, <span class="at">none =</span> <span class="st">"No Evaluation"</span>, <span class="at">random =</span> <span class="st">"Random (Bayesian)"</span>, <span class="at">freq =</span> <span class="st">"Random (Frequentist)"</span>, <span class="at">evalsecond =</span> <span class="st">"Evaluate Second Best (Bayesian)"</span>,</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>                 <span class="at">freq_evalsecond =</span> <span class="st">"Evaluate Second Best (Frequentist)"</span>, <span class="at">pftdpw =</span> <span class="st">"PFT-DPW"</span>, <span class="at">best =</span> <span class="st">"Hypothetical Best"</span>)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="in">```{julia}</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: load-sim-data</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>all_sim_data = deserialize(<span class="ot">"</span><span class="st">temp-data/sim</span><span class="wa">$(</span><span class="st">sim_file_suffix).jls</span><span class="ot">"</span>) </span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>The main goal of this simulation study is to analyze the sequential decision problem encountered by organizations involved in evaluating and funding charities from the perspective of Effective Altruism, which seeks to maximize the positive impact of donations on a global scale. In particular, the study aims to compare different decision-making policies for two key tasks:</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>(i) Selecting programs to fund from a list of programs for which effectiveness is only partially observable, taking into account the inherent uncertainties in program outcomes and impact.</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>(ii) Determining which programs to re-evaluate in order to incrementally improve the decision-making process for program selection (i), by updating information and adjusting funding allocations accordingly.</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>By investigating and evaluating various decision-making policies within this framework, the study aims to contribute insights into how organizations can make more informed and effective funding decisions, with the ultimate goal of maximizing positive impact and optimizing resource allocation for charitable purposes.</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>My objective is not to identify the optimal policy, but rather to explore the potential for welfare improvement using alternative policies to those conventionally used. It's important to note that I am simplifying these policies for tractability and not considering all their complexities and context-specific adjustments that expert decision-makers may introduce. Nevertheless, I believe this study captures the essence of how conventionally used policies may underperform in certain scenarios.</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>Specifically, I aim to highlight the limitations of the following policies: (a) never re-evaluating programs and relying solely on initial evaluations, (b) randomly re-evaluating programs, and (c) using null hypothesis significance testing (NHST) in a simple heuristic policy. I will compare these conventional policies against policies that utilize a partially observable Markov decision process (POMDP) algorithm and a simple heuristic policy that uses Bayesian hierarchical models. Through my analysis, I have found that the alternative policies are able to increase accumulated discounted utility by at least 20 percent after a few steps. </span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>Furthermore, it is important to highlight that while the framework of the implementation-evaluation problem in this study draws inspiration from the decision-making challenges faced by funding organizations in the realm of international development and global health charities, it is also relevant to the broader context of Effective Altruism. The decision problems faced by Effective Altruism practitioners often involve complex trade-offs and uncertainties, and the insights gained from this study may have broader implications for decision-making in these domains as well.</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>The funder's problem is modeled as a sequence of decisions made at discrete intervals, given a finite set of programs with uncertain<span class="co">[</span><span class="ot">Focusing on epistemic uncertainty and ignoring moral uncertainty.</span><span class="co">]</span>{.aside} impact on a set of populations. The funder selects optimal programs to implement based on their beliefs about the counterfactual outcomes of these programs for their targeted populations, and decides what data to collect to update these beliefs for the next decision point. The environment and problem are intentionally kept simple to ensure tractability, with the understanding that further studies may revisit these assumptions iteratively.</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>Thus the problem is modeled as a bandit problem, but without the restriction of only being able to evaluate implemented programs. Each program is assumed to target a particular population without any overlap, and the cost of implementation is held fixed and equal for all programs. There are no new programs entering the problem over time. The state of each program varies over time and is drawn from a hierarchical and stationary program hyperstate, which determines the data generating process for observed data when a program is evaluated.<span class="co">[</span><span class="ot">_State_ here refers to the causal model determining outcome counterfactuals depending on whether a program is implemented or not. It is the data generating process from which we observe data when a program is evaluated.</span><span class="co">]</span>{.aside} </span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>While the optimal method to select a program for implementation is a probabilistic one, taking into account the distribution of counterfactual quantities and any available prior information, I also consider the commonly used null hypothesis significance testing (NHST) approach.<span class="ot">[^bayes-vs-freq]</span> However, my focus is not on comparing the probabilistic and NHST decision rules, but rather on the sequential nature of these decisions in the presence of heterogeneity in program effectiveness. I aim to examine the potential to improve welfare by enhancing the planning scheme used to select programs for re-evaluation, which I refer to as the _meta-problem_.</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Explain how the time hierarchy is similar to the context one. --&gt;</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="ot">[^bayes-vs-freq]: </span>Given a risk-neutral utility function and very weakly informed priors, both these methods are often assumed to result in very similar decisions. However, a winning entry in <span class="co">[</span><span class="ot">GiveWell's</span><span class="co">](http://givewell.com)</span> <span class="co">[</span><span class="ot">Change Our Minds Context</span><span class="co">](https://blog.givewell.org/2022/12/15/change-our-mind-contest-winners/)</span> by @Haber2022 showed that threshold-based method, like NHST, suffers from bias caused by the winner's curse phenomenon. A big difference between what I am investigating here and Haber's work is that I am looking beyond the one-shot accept/reject decision.</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="fu"># The Environment</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>As mentioned previously, in this study, a simplified environment is utilized while striving to capture the most relevant aspects of the real-world context. The funder is assumed to be faced with a set of programs, denoted as $\mathcal{K}$, and must decide which program(s) to fund and which program(s) to re-evaluate.<span class="co">[</span><span class="ot">$\mathcal{K} = \{1,\ldots,K\}$. In this study, $K = 10$.</span><span class="co">]</span>{.aside} This decision needs to be made repeatedly over a series of steps.</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>The environment is modeled as a multi-armed bandit (MAB) framework, where each program or intervention is represented as a bandit with a stochastic causal model. In the sequential environment, at each step, a new _state_ is drawn from a _hyperstate_ that determines the outcomes of the targeted population. This hyperstate is used to simulate the underlying uncertainty and variability of real-world interventions, capturing the inherent uncertainties in program outcomes and their effects on the population.[This is a broad simplification. In reality, we would distinguish between _programs_ and _populations_; different programs can be effective in different populations and a program could simultaneously target different populations.]{.aside}</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>By employing a MAB framework and incorporating hyperstates, this study aims to capture the dynamic nature of decision-making in funding programs, where the funder must adapt and update their choices over time based on changing states and outcomes. This approach allows for exploring different decision-making policies and their impact on program selection and re-evaluation, in order to optimize resource allocation and improve the effectiveness of charitable funding decisions. </span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>For each program $k$, we model the data generating process for each individual's outcome at step $t$ as,</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>  Y_{t}(z) &amp;\sim \mathtt{Normal}(\mu_{k<span class="co">[</span><span class="ot">i</span><span class="co">]</span>,t} + z\cdot \tau_{k<span class="co">[</span><span class="ot">i</span><span class="co">]</span>,t}, \sigma_{k<span class="co">[</span><span class="ot">i</span><span class="co">]</span>}) <span class="sc">\\</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>  \mu_{kt} &amp;\sim \mathtt{Normal}(\mu_k, \eta^\mu_k) <span class="sc">\\</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>  \tau_{kt} &amp;\sim \mathtt{Normal}(\tau_k, \eta^\tau_k)</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">For simplicity, $\sigma_k$ is homoskedastic and does not vary over time.</span><span class="co">]</span>{.aside}</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>where $z$ is a binary variable indicating whether a program is implemented or not, which means $\tau_{kt}$ is the average treatment effect. We therefore denote the state of a program to be $\boldsymbol{\theta}_{kt} = (\mu_{kt}, \tau_{kt}, \sigma_k)$. </span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>On the other hand, the hyperstate for each program, $\boldsymbol{\theta}_k = (\mu_k, \tau_k, \sigma_k, \eta^\mu_k, \eta^\tau_k)$, is drawn from the prior </span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>  \mu_k &amp;\sim \mathtt{Normal}(0, \xi^\mu) <span class="sc">\\</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>  \tau_k &amp;\sim \mathtt{Normal}(0, \xi^\tau) <span class="sc">\\</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>  \sigma_k &amp;\sim \mathtt{Normal}^+(0, \xi^\sigma) <span class="sc">\\</span></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>  \eta^\mu_k &amp;\sim \mathtt{Normal}^+(0, \xi^{\eta^\mu}) <span class="sc">\\</span></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>  \eta^\tau_k &amp;\sim \mathtt{Normal}^+(0, \xi^{\eta^\tau}), <span class="sc">\\</span></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>where $\boldsymbol{\xi} = (\xi^\mu, \xi^\tau, \xi^\sigma, \xi^{\eta^\mu}, \xi^{\eta^\tau})$ are the hyperparameters of the environment. This means that while each program has a fixed average baseline outcome, $\mu_k$, and average treatment effect, $\tau_k$, at every step, normally distributed shocks alter the realized averages. <span class="co">[</span><span class="ot">With some abuse of notation, I will write $\boldsymbol{\theta_{kt}\sim\theta_k}$ and $\boldsymbol{\theta_k\sim\boldsymbol{\xi}}$.</span><span class="co">]</span>{.aside} </span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>In this environment, the hierarchical structure of the hyperstate represents the inherent heterogeneity of program effectiveness over time, highlighting the limitations of relying solely on a single evaluation of a program at a particular point in time. The assumption is made that this variation in effectiveness follows a purely oscillatory pattern without any trends. While funders should also be concerned about variations in effectiveness when programs are implemented in different contexts<span class="co">[</span><span class="ot">Context here refers to geography or populations. Meta-analyses are typically aimed at understanding the generalizability of evaluations between contexts.</span><span class="co">]</span>{.aside}, this aspect is ignored in this simplified environment, assuming that the time variation captures the general problem of heterogeneity over time and context. As the states in the hyperstate vary randomly and independently, the objective of the funder is to learn about the underlying hyperstate, rather than predicting the next realized state.<span class="co">[</span><span class="ot">Future iterations of this model could introduce some correlation between states over time.</span><span class="co">]</span>{.aside} </span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{julia}</span></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: states-example-data</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>ep1_states = <span class="dt">@pipe</span> [<span class="dt">@transform</span>!(DataFrame(<span class="ot">s.programstates</span><span class="ch">)</span><span class="ot">, :t = t</span><span class="ch">)</span><span class="ot"> for </span><span class="ch">(</span><span class="ot">t,s</span><span class="ch">)</span><span class="ot"> in enumerate</span><span class="ch">(</span><span class="ot">all_sim_data.</span><span class="st">state[1])] |&gt; </span></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a><span class="st">  vcat(_</span><span class="ot">.</span>..) |&gt; </span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(_, Not(:progdgp))</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-states-example</span></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Population outcomes over time for 10 example programs. Ribbons represent the mean outcome $\\pm \\sigma_p$."</span></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="co">#| cap-location: margin</span></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>ep1_states <span class="ot">&lt;-</span> <span class="fu">julia_eval</span>(<span class="st">"ep1_states"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transmute</span>(programid, t, <span class="at">outcome_control =</span> μ, <span class="at">outcome_treated =</span> outcome_control <span class="sc">+</span> τ, <span class="at">sd =</span> σ) <span class="sc">|&gt;</span> </span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">starts_with</span>(<span class="st">"outcome_"</span>), <span class="at">names_to =</span> <span class="st">"z"</span>, <span class="at">names_prefix =</span> <span class="st">"outcome_"</span>, <span class="at">values_to =</span> <span class="st">"outcome"</span>) </span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>ep1_states <span class="sc">|&gt;</span> </span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(t <span class="sc">&lt;=</span> <span class="dv">15</span>) <span class="sc">|&gt;</span>  </span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(t, outcome)) <span class="sc">+</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">color =</span> z)) <span class="sc">+</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> outcome <span class="sc">-</span> sd, <span class="at">ymax =</span> outcome <span class="sc">+</span> sd, <span class="at">fill =</span> z), <span class="at">alpha =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_discrete</span>(<span class="st">""</span>, <span class="at">labels =</span> str_to_title, <span class="at">aesthetics =</span> <span class="fu">c</span>(<span class="st">"color"</span>, <span class="st">"fill"</span>)) <span class="sc">+</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Program Outcomes"</span>, <span class="at">x =</span> <span class="st">""</span>, <span class="at">y =</span> <span class="st">"Y"</span>) <span class="sc">+</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(programid), <span class="at">ncol =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>)</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>The funder is never aware of the true state of the world --- the true counterfactual model of all programs' effectiveness --- but they are able to evaluate a program by collecting data and updating their beliefs. I assume that the funder has an initial observation for each program under consideration. This could be data from an earlier experiment or could represent the funder's or other experts' prior beliefs.</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-utility-fig</span></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: The exponential utility function, $U(y;\alpha) = 1 - e^{- \alpha y},$ where $\alpha$ represents the degree of risk aversion. In this study, we have $\alpha = 0.25$.</span></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap-location: bottom</span></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 3</span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 3</span></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="co">#| column: margin</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>utility <span class="ot">&lt;-</span> <span class="cf">function</span>(c, alpha) <span class="dv">1</span> <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span>alpha <span class="sc">*</span> c)</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>expected_utility <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sd, alpha) <span class="dv">1</span> <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span>alpha <span class="sc">*</span> mu <span class="sc">+</span> alpha<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> sd<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a><span class="fu">crossing</span>(<span class="at">a =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="fl">0.125</span><span class="sc">/</span><span class="dv">2</span>), <span class="at">c =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="fl">0.1</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">u =</span> <span class="fu">utility</span>(c, a)) <span class="sc">|&gt;</span> </span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(c, u)) <span class="sc">+</span></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> \(x) <span class="fu">filter</span>(x, a <span class="sc">==</span> <span class="fl">0.25</span>)) <span class="sc">+</span></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> a), <span class="at">alpha =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"y"</span>, <span class="at">y =</span> <span class="st">"U(y)"</span>) <span class="sc">+</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="fl">0.5</span>)) <span class="sc">+</span></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>  <span class="cn">NULL</span></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a><span class="in">```</span> </span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>In the evaluation of which program to implement, the agent is assumed to be maximizing welfare, which is measured using a utility function. The program outcomes, as mentioned earlier, are represented in terms of an abstract quantity, such as income. By incorporating a utility function, the analysis takes into account the possibility of risk aversion and diminishing marginal utility, recognizing that it may be more optimal to prioritize increasing the utility of individuals with lower baseline utility, even if it has relatively lower cost-effectiveness, or to choose programs with lower uncertainty. The utility function used in this study is the _exponential utility function_.</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>When there is uncertainty or variability in the outcomes of different programs, it is important to work with expected utilities to account for this variability. For instance, if we have information on the means and standard deviations of outcomes over time, denoted as $\mu_{kt} + z\cdot \tau_{kt}$ and $\sigma_k$, respectively, the expected utility can be calculated as in @fig-state-util-example. </span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-state-util-example </span></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Population expected utility over time for 10 example programs. $E_{Y_{kt}\\sim\\boldsymbol{\\theta_{kt</span><span class="re">}}}</span><span class="co">[U(Y_{kt}(z))] = 1 - e^{-\\alpha(\\mu_{kt} + z\\cdot\\tau_{kt}) + \\alpha^2 \\sigma_k^2/2}$."</span></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a><span class="co">#| cap-location: margin</span></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>ep1_states <span class="sc">|&gt;</span> </span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">eu =</span> <span class="fu">expected_utility</span>(outcome, sd, <span class="fl">0.25</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(t <span class="sc">&lt;=</span> <span class="dv">15</span>) <span class="sc">|&gt;</span>  </span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(t, eu)) <span class="sc">+</span></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">color =</span> z)) <span class="sc">+</span></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_discrete</span>(<span class="st">""</span>, <span class="at">labels =</span> str_to_title, <span class="at">aesthetics =</span> <span class="fu">c</span>(<span class="st">"color"</span>, <span class="st">"fill"</span>)) <span class="sc">+</span></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Program Expected Utility"</span>, <span class="at">x =</span> <span class="st">""</span>, <span class="at">y =</span> <span class="st">"E[U(Y)]"</span>) <span class="sc">+</span></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(programid), <span class="at">ncol =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>)</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a><span class="fu"># The Problem {#sec-problem}</span></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>Now that the environment in which the funder operates has been described, the problem they are trying to solve can be addressed. The funder is confronted with a set of $K$ programs and must make two decisions, taking two actions:</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>(i) Select one program to fund (i.e., to implement) or none.</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>(ii) Select one program to evaluate or none.</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>At every time step $t$, the agent must choose a tuple $(m,v)$ from the action set $$\mathcal{A} = <span class="sc">\{</span>(m,v): m, v\in \mathcal{K}\cup<span class="sc">\{</span>0<span class="sc">\}\}</span>,$$ where $m$ represents the program to be funded (with $0$ representing no program), and $v$ represents the program to be evaluated (with $0$ representing no evaluation).</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>This presents a simpler problem than is typical of a multi-armed bandit problem; there is no real trade-off to make here between choosing the optimal program to fund and gathering more information on which is the optimal program. Nevertheless, we are confronted by an _evaluative_ problem such that we must choose how to gather information most effectively. Furthermore, while a typical multi-armed bandit problem is not viewed as _sequential_ in the sense that an action at any step does not change future states, we can reformulate our problem to use the funder's _beliefs_ about parameters of the programs' causal models as the state <span class="co">[</span><span class="ot">@Morales2020;@Kochenderfer2022</span><span class="co">]</span>. </span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>In that case, the problem is now a _Markov decision process_ (MDP). The agent needs a _policy_, $\pi(b)$, that selects what action to take given the belief $b_t(\boldsymbol{\theta})$ over the continuous space of possible states.[Let the states of all the programs be $\boldsymbol{\theta}_t = (\boldsymbol{\theta}_{kt})_{k\in\mathcal{K}}$.]{.aside} Putting this together we get the _state-value_ function</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>\begin{equation*}</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>V_\pi(b_{t-1}) &amp;= \int_{\Theta,\mathcal{O}} \left<span class="co">[</span><span class="ot">R(a_t, \boldsymbol{\theta}) + \gamma V_\pi(b_{t})\right</span><span class="co">]</span>p(o\mid\boldsymbol{\theta}, a_t)b_{t-1}(\boldsymbol{\theta})\,\textrm{d}\boldsymbol{\theta}\textrm{d}o <span class="sc">\\</span> <span class="sc">\\</span></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>a_t &amp;= \pi(b_t) <span class="sc">\\</span></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>R(a, \boldsymbol{\theta}) &amp;= E_{Y\sim\boldsymbol{\theta}}<span class="co">[</span><span class="ot">U(Y(a))</span><span class="co">]</span> = \sum_{k\in\mathcal{K}} E_{Y_k\sim\boldsymbol{\theta}_k}\left[U(Y_{k}(a^m_k))\right], </span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>\end{equation*}</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>$${#eq-problem}<span class="co">[</span><span class="ot">In this simulation study we set the discount rate to $\gamma = 0.95$.</span><span class="co">]</span>{.aside}</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>where $o \in \mathcal{O}$ is the data collected based on the evaluation action for a particular program, and using it we update $b_{t-1}$ to $b_{t}$.</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>So given the current belief $b_{t-1}$ and the policy $\pi$, the agent estimates both the immediate reward and future discounted rewards -- given an updated belief $b_{t}$ continguent on the data collected $o$ -- and so forth recursively. Based on this the accumulated returns would be </span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>G_{\pi,t:T} = \sum_{r=t}^T \gamma^{r-t}E_{\boldsymbol{\theta}_r\sim b_{r-1}}[R(\pi(b_{r-1}), \boldsymbol{\theta}_r)],</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>where $T$ is the terminal step.<span class="co">[</span><span class="ot">In this study, I use $T = 15$.</span><span class="co">]</span>{.aside}  </span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>Unlike a typical MDP, the agent in this case does not observe the actual realized reward at each step, but must estimate it conditional on their beliefs. Program implementers do not automatically receive a reliable signal on the observed and counterfactual rewards. This is an important aspect of the funder's problem: while in an MDP, we would normally observe a reward for the selected action, or some noisy version of it; in the funder's environment, all rewards are inferred.<span class="co">[</span><span class="ot">Also different from a MDP: we receive utility from every program, or rather from the population it targets.</span><span class="co">]</span>{.aside}</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a><span class="fu"># The Plans</span></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>Now, let's discuss the policies that will be evaluated as part of the funder's meta-problem:</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>_No evaluation_, where we never re-evaluate any of the programs and only use our initial beliefs, denoted as $b_0$, to decide which program to implement.</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>_Random evaluation_, where at every time step $t$, we randomly select one of the $K$ programs to be evaluated. For example, this happens if studies are conducted by researchers in an unplanned manner. </span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>_Evaluate second-best_, where at every time step $t$, we select the program that has the second highest estimated reward for evaluation.</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>_Particle Filter Tree with Progressive Depth Widening (PFT-DPW)_, where we use an offline Monte Carlo Tree Search (MCTS) policy variant, to select the program to evaluate <span class="co">[</span><span class="ot">@Sunberg2018</span><span class="co">]</span>.<span class="ot">[^pftdpw]</span></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>For all the policies being experimented with, we maintain a belief about the expected utility of implementation counterfactuals. We use a hierarchical Bayesian posterior to represent our updated beliefs for all the policies. For the PFT-DPW policy, we use a particle filter to efficiently manage these beliefs as we iteratively build a tree of action-observation-belief trajectories.</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a><span class="in">```{julia}</span></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: rewards-and-actions </span></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>all_rewards = <span class="dt">@pipe</span> all_sim_data |&gt; </span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>    <span class="dt">@subset</span>(_, :plan_type .== <span class="ot">"</span><span class="st">none</span><span class="ot">"</span>) |&gt; </span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>    get_rewards_data.(_.<span class="kw">state</span>, Ref(actlist), Ref(util_model)) |&gt;</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>    [<span class="dt">@transform</span>!(rd[<span class="dv">2</span>], :sim = rd[<span class="dv">1</span>]) <span class="kw">for</span> rd in enumerate(_)] |&gt;</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>    vcat(_...) |&gt;</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>    insertcols!(_, :reward_type =&gt; <span class="ot">"</span><span class="st">actual</span><span class="ot">"</span>)</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>obs_act = <span class="dt">@pipe</span> all_sim_data |&gt; </span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>  <span class="dt">@rsubset</span>(_, :plan_type in [<span class="ot">"</span><span class="st">pftdpw</span><span class="ot">"</span>, <span class="ot">"</span><span class="st">random</span><span class="ot">"</span>, <span class="ot">"</span><span class="st">freq</span><span class="ot">"</span>, <span class="ot">"</span><span class="st">evalsecond</span><span class="ot">"</span>, <span class="ot">"</span><span class="st">freq_evalsecond</span><span class="ot">"</span>]) |&gt; </span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>  groupby(_, :plan_type) |&gt; </span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>  combine(_, d -&gt; vcat(get_actions_data.(d.action)..., source = :sim)) </span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ex-ante-reward</span></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>all_rewards <span class="ot">&lt;-</span> <span class="fu">julia_eval</span>(<span class="st">"all_rewards"</span>) </span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>obs_act <span class="ot">&lt;-</span> <span class="fu">julia_eval</span>(<span class="st">"obs_act"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">plan_type =</span> <span class="fu">factor</span>(plan_type, <span class="at">levels =</span> <span class="fu">names</span>(plan_labels)))</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>ex_ante_reward_data <span class="ot">&lt;-</span> all_rewards <span class="sc">|&gt;</span> </span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(step <span class="sc">==</span> maxstep) <span class="sc">|&gt;</span> </span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">!</span>step) <span class="sc">|&gt;</span> </span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(sim) <span class="sc">|&gt;</span> </span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>    <span class="at">ex_ante_best =</span> ex_ante_reward <span class="sc">&gt;=</span> <span class="fu">max</span>(ex_ante_reward),</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>    <span class="at">reward_rank =</span> <span class="fu">min_rank</span>(ex_ante_reward) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">|&gt;</span> </span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()  </span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-actions</span></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Evaluate and implement actions over $K = 15$ steps for five example episodes (rows). For each episode, we observe how the different policies behave (columns). The plot has been arranged such that the y-axis is in ascending order of _ex ante_ optimality."</span></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap-location: margin</span></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>obs_act <span class="sc">|&gt;</span></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">between</span>(sim, <span class="dv">1</span>, <span class="dv">5</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">c</span>(implement_programs, eval_programs), <span class="at">names_to =</span> <span class="st">"action_type"</span>, <span class="at">names_pattern =</span> r<span class="st">"{(.+)_programs}"</span>, <span class="at">values_to =</span> <span class="st">"pid"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(ex_ante_reward_data, <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"sim"</span>, <span class="st">"pid"</span> <span class="ot">=</span> <span class="st">"actprog"</span>)) <span class="sc">|&gt;</span></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(step, reward_rank, <span class="at">color =</span> action_type)) <span class="sc">+</span></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.85</span>) <span class="sc">+</span></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">"Step"</span>, <span class="at">breaks =</span> <span class="fu">seq</span>(maxstep)) <span class="sc">+</span></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">""</span>, <span class="at">breaks =</span> <span class="dv">0</span><span class="sc">:</span>nprograms, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_discrete</span>(<span class="st">"Action Type"</span>, <span class="at">labels =</span> <span class="fu">c</span>(<span class="at">eval =</span> <span class="st">"Evaluation"</span>, <span class="at">implement =</span> <span class="st">"Implementation"</span>)) <span class="sc">+</span></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="at">cols =</span> <span class="fu">vars</span>(plan_type), <span class="at">rows =</span> <span class="fu">vars</span>(sim), <span class="at">scales =</span> <span class="st">"free_y"</span>, <span class="at">labeller =</span> <span class="fu">labeller</span>(<span class="at">plan_type =</span> plan_labels)) <span class="sc">+</span></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">panel.grid.minor =</span> <span class="fu">element_blank</span>(), <span class="at">axis.text =</span> <span class="fu">element_blank</span>(), <span class="at">legend.position =</span> <span class="st">"top"</span>, <span class="at">strip.text.y.right =</span> <span class="fu">element_blank</span>(), <span class="at">strip.text.x.top =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">7</span>),</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.ticks =</span> <span class="fu">element_blank</span>())</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>For the random and evaluate-second-best policies, I also consider a simple frequentist NHST approach. This involves running a regression on all the observed data, testing whether the treatment effect is statistically significant at the 10 percent level, and assuming the point estimate to be the true treatment effect if it is statistically significant, and assuming it to be zero otherwise. It's important to note that using frequentist inference in this context essentially ignores uncertainty, but we still use the expected utility based on $\sigma$. This form of inference is intended to highlight the limitations of binary decision-making based solely on statistical significance tests or arbitrary thresholds, instead of quantifying uncertainty. Although this approach is a simplification, it helps keep the argument intuitive.</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- [GiveWell in fact look at point estimates of cost-effectiveness and use a threshold of some multiple of the cost-effectiveness of GiveDirectly, a cash transfer program. They also use subjective adjustments to the point estimates to account for uncertainty.]{.aside}   --&gt;</span></span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>The motivation for selecting these policies/algorithms/heuristics is not to determine an optimal one, but rather to compare and contrast commonly used approaches. Specifically, the frequentist no-evaluation and random policies are chosen as they closely resemble the practices often employed by funders and implementers in real-world scenarios.</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>So given this set of policies, $\Pi$, the meta-problem that we want to solve is choosing the best policy,</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>\max_{\pi \in \Pi} W_T(\pi) = E_{\boldsymbol{\theta}\sim\boldsymbol{\xi}}\left<span class="sc">\{</span> \sum_{t=1}^T\gamma^{t-1}E_{\boldsymbol{\theta}_t\sim\boldsymbol{\theta}}[R(\pi(b_t), \boldsymbol{\theta}_t)] \right<span class="sc">\}</span>. </span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>$${#eq-meta-problem}</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>Notice how this differs from the funder's problem in @eq-problem: here we assume we know the hyperstates and states which we draw from the prior, $\boldsymbol{\xi}$, not from beliefs, $b$.</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a><span class="ot">[^pftdpw]: </span>The PFT-DPW algorithm is a hybrid approach for solving partially observable Markov decision processes (POMDPs) that combines particle filtering and tree-based search. It represents belief states using a tree data structure and uses double progressive widening to selectively expand promising regions of the belief state space. Particle weights are used to represent the probabilities of different belief states, and these weights are updated through the particle filtering and tree expansion process. Actions are selected based on estimated belief state values, and the tree is pruned to keep it computationally efficient.   </span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a><span class="fu"># Results</span></span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a><span class="in">```{julia}</span></span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: prepare-util-data</span></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>do_nothing_reward = <span class="dt">@pipe</span> <span class="dt">@subset</span>(all_sim_data, :plan_type .== <span class="ot">"</span><span class="st">none</span><span class="ot">"</span>) |&gt; </span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a>  get_do_nothing_plan_data(_, util_model) </span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a>do_best_reward = <span class="dt">@pipe</span> <span class="dt">@subset</span>(all_sim_data, :plan_type .== <span class="ot">"</span><span class="st">none</span><span class="ot">"</span>) |&gt;</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>    dropmissing(_, :<span class="kw">state</span>) |&gt; </span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>    <span class="dt">@select</span>(</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a>        _,</span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>        :actual_reward = <span class="fu">map</span>(get_program_reward, :<span class="kw">state</span>),</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>        :actual_ex_ante_reward = <span class="fu">map</span>(<span class="ot">s</span> <span class="ot">-&gt; get_program_reward</span><span class="ch">(</span><span class="ot">s, eval_getter = dgp</span><span class="ch">)</span><span class="ot">, :state</span><span class="ch">)</span><span class="ot">,</span></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a><span class="ot">        :plan_type = "best"</span></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a><span class="ot">    </span><span class="ch">)</span></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a><span class="ot">util_data = </span><span class="dt">@pipe</span><span class="ot"> all_sim_data </span><span class="ch">|</span><span class="ot">&gt; </span></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a><span class="ot">    vcat</span><span class="ch">(</span><span class="ot">_, do_best_reward, do_nothing_reward, cols = :union</span><span class="ch">)</span><span class="ot"> </span><span class="ch">|</span><span class="ot">&gt; </span></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a><span class="ot">    </span><span class="dt">@select</span><span class="ot">!</span><span class="ch">(</span><span class="ot">_, :plan_type, :actual_reward, :actual_ex_ante_reward, :step = repeat</span><span class="ch">([</span><span class="bn">collect(1:maxstep)</span><span class="ch">]</span><span class="ot">, length</span><span class="ch">(</span><span class="ot">:plan_type</span><span class="ch">)))</span><span class="ot"> </span><span class="ch">|</span><span class="ot">&gt; </span></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a><span class="ot">    groupby</span><span class="ch">(</span><span class="ot">_, :plan_type</span><span class="ch">)</span><span class="ot"> </span><span class="ch">|</span><span class="ot">&gt; </span></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a><span class="ot">    transform!</span><span class="ch">(</span><span class="ot">_, eachindex =&gt; :sim</span><span class="ch">)</span><span class="ot"> </span><span class="ch">|</span><span class="ot">&gt; </span></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a><span class="ot">    flatten</span><span class="ch">(</span><span class="ot">_, Not</span><span class="ch">([</span><span class="bn">:plan_type, :sim</span><span class="ch">]))</span></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a><span class="ot">```</span></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a><span class="st">#| label: util-diff-data</span></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a><span class="st">#| include: false</span></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a><span class="st">util_data &lt;- julia_eval("util_data") |&gt; </span></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a><span class="st">  mutate(plan_type = factor(plan_type, levels = names(plan_labels)))</span></span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a><span class="st">n_episodes &lt;- filter(util_data, step == 1, fct_match(plan_type, "none")) |&gt; nrow()</span></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="st">vn_util_diff &lt;- util_data |&gt; </span></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a><span class="st">  unnest(c(actual_reward, actual_ex_ante_reward)) |&gt; </span></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a><span class="st">  pivot_longer(!c(sim, plan_type, step), names_to = "reward_type", names_pattern = r"{actual_(.*)_reward}", values_to = "reward") |&gt; </span></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a><span class="st">  mutate(reward_type = coalesce(reward_type, "ex_post")) %&gt;%  </span></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a><span class="st">  left_join(filter(., fct_match(plan_type, "no impl")) |&gt; select(!plan_type), by = c("reward_type", "sim", "step"), suffix = c("", "_no_impl")) |&gt; </span></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a><span class="st">  filter(!fct_match(plan_type, "no impl")) |&gt; </span></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a><span class="st">  mutate(reward_diff = reward - reward_no_impl) |&gt; </span></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a><span class="st">  arrange(step) |&gt; </span></span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a><span class="st">  group_by(plan_type, reward_type, sim) |&gt; </span></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a><span class="st">  mutate(</span></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a><span class="st">    discounted_reward_diff = (discount^(step - 1)) * reward_diff,</span></span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a><span class="st">    accum_reward_diff = cumsum(reward_diff),</span></span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a><span class="st">    discounted_accum_reward_diff = cumsum(discounted_reward_diff)</span></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a><span class="st">  ) |&gt; </span></span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a><span class="st">  ungroup() |&gt; </span></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a><span class="st">  pivot_longer(c(reward_diff, discounted_reward_diff, accum_reward_diff, discounted_accum_reward_diff), values_to = "reward_diff") |&gt; </span></span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a><span class="st">  mutate(</span></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a><span class="st">    accum = str_detect(name, fixed("accum")),</span></span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a><span class="st">    discounted = str_detect(name, fixed("discounted"))</span></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a><span class="st">  ) |&gt; </span></span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a><span class="st">  select(!name)</span></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a><span class="ot">```</span></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>In this simulation experiment, we run a total of <span class="dt">$S</span> = r n_episodes$ episodes.^[Why <span class="ot">not</span> more? Each simulated episode can be <span class="fu">time</span><span class="ot">-c</span>onsuming, especially <span class="kw">when</span> using the PFT-DPW policy which involves <span class="dv">1</span>,<span class="dv">000</span> iterations at every step before selecting a program <span class="kw">for</span> evaluation. Even simpler policies, such as the random policies, take <span class="fu">time</span> <span class="kw">when</span> updating beliefs using a Bayesian model that fits all the observed data <span class="kw">for</span> a program at every evaluation.] For <span class="kw">each</span> episode, we draw <span class="dt">$K</span>$ hyperstates from the prior, denoted as <span class="wa">$\</span><span class="dt">boldsymbol</span>{\theta}_s\sim\boldsymbol{\xi}<span class="wa">$,</span> <span class="ot">and</span> then <span class="kw">for</span> <span class="kw">each</span> step within the episode, we draw states denoted as <span class="wa">$\</span><span class="dt">boldsymbol</span>{\theta}_{st}\sim\boldsymbol{\theta}_s<span class="wa">$.</span> Next, we apply <span class="kw">each</span> of <span class="kw">our</span> policies to this episode, making decisions on which programs to implement <span class="ot">and</span> which ones to evaluate in order to update beliefs <span class="dt">$b</span>{st}<span class="wa">$.</span> This allows us to observe the trajectory of <span class="wa">$(</span><span class="dt">b_</span>{<span class="ot">s,0},</span><span class="st"> a_{s</span><span class="ot">,</span><span class="dv">1</span>}, o_{<span class="ot">s,1},</span><span class="st"> b_{s</span><span class="ot">,</span><span class="dv">1</span>}, a_{<span class="ot">s,2},</span><span class="st"> o_{s</span><span class="ot">,</span><span class="dv">2</span>}, b_{<span class="ot">s,2},</span><span class="ch">\l</span><span class="st">dots)$ for each policy</span><span class="ot">,</span> <span class="kw">given</span> the same states <span class="ot">and</span> hyperstates.[We actually solve <span class="dt">@eq</span>-meta<span class="ot">-p</span>roblem as </span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a><span class="wa">$$</span></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>\max_{\pi \in \Pi} \widetilde{W}_T(\pi) = \frac{<span class="dv">1</span>}{S} \sum_{s=<span class="dv">1</span>}^S \sum_{t=<span class="dv">1</span>}^T\gamma^{t<span class="dv">-1</span>}R(\pi(b_{st}), \boldsymbol{\theta}_{st}). </span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a><span class="wa">$$</span>]{.aside}  </span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>To assess the performance of the policies, we compare their mean accumulated discounted utility to the same quantity <span class="kw">when</span> none of the programs are implemented. In Figure <span class="dt">@fig</span><span class="ot">-r</span>eturns<span class="ot">-c</span>ompare, we can observe how this difference evolves over the <span class="dt">$T</span>$ steps of all the simulation episodes. We can see that the PFT-DPW <span class="ot">and</span> Bayesian evaluate<span class="ot">-s</span>econd<span class="ot">-b</span>est policies perform the best, with higher accumulated discounted utility compared to other policies. The frequentist policies <span class="ot">and</span> the random Bayesian policy show lower performance. The <span class="fu">no</span><span class="ot">-e</span>valuation policy, where decisions are made based only on the initial belief <span class="dt">$b_0</span><span class="wa">$,</span> performs the worst among all the policies.</span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a><span class="st">#| label: fig-returns-compare</span></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-cap: Mean accumulated discounted utility gains, compared to a no program implementation policy, </span><span class="wa">$$</span><span class="ch">\w</span><span class="st">idetilde{W}_T(\pi) - </span><span class="ch">\w</span><span class="st">idetilde{W}_T(\pi^</span><span class="ch">\e</span><span class="st">mptyset),</span><span class="wa">$$</span><span class="st"> where </span><span class="wa">$\</span><span class="st">pi^</span><span class="ch">\e</span><span class="st">mptyset(b) = (0,0),</span><span class="ch">\f</span><span class="st">orall b.$ </span></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-cap-location: margin</span></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a><span class="st">vn_util_diff |&gt;</span></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a><span class="st">  filter(accum, discounted, fct_match(plan_type, c("pftdpw", "freq", "random", "none", "evalsecond", "freq_evalsecond")), fct_match(reward_type, "ex_post")) |&gt; </span></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a><span class="st">  ggplot(aes(step)) +</span></span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a><span class="st">  tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Mean"), .width = 0.0, linewidth = 0.25, point_interval = mean_qi) +</span></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a><span class="st">  # tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Median"), .width = 0.0, linewidth = 0.25, point_interval = median_qi) +</span></span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_x_continuous("Step", breaks = seq(maxstep)) +</span></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_y_continuous("Mean Accumulated Utility Gains", breaks = seq(0, 2, 0.1)) +</span></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a><span class="st">  #scale_linetype_manual("", values = c("Mean" = "dashed", "Median" = "solid")) +</span></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_color_discrete(</span></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a><span class="st">    "Policy", </span></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a><span class="st">    labels = plan_labels, </span></span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a><span class="st">    aesthetics = c("color", "fill")</span></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a><span class="st">  ) +</span></span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a><span class="st">  # facet_wrap(vars(reward_type), ncol = 1, scales = "free_y", labeller = as_labeller(c(ex_ante = "ex ante", ex_post = "ex post"))) +</span></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a><span class="st">  # labs(title = "Accumulated Utility Improvement Compared to No Implementation") +</span></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a><span class="st">  theme(panel.grid.minor.x = element_blank()) +</span></span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a><span class="st">  guides(linetype = "none") +</span></span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a><span class="st">  NULL</span></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a><span class="ot">```</span></span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a><span class="st">#| label: fig-returns-percent-compare</span></span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-width: 3</span></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-height: 3</span></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-cap: Percentage increase in mean accumulated discounted utility gain, </span><span class="wa">$\</span><span class="st">frac{</span><span class="ch">\w</span><span class="st">idetilde{W}_T(\pi) - </span><span class="ch">\w</span><span class="st">idetilde{W}_T(\pi')}{</span><span class="ch">\w</span><span class="st">idetilde{W}_T(\pi')}.$</span></span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-cap-location: bottom</span></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a><span class="st">#| column: margin </span></span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a><span class="st">vn_util_diff |&gt;</span></span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a><span class="st">  filter(accum, discounted, fct_match(plan_type, c("pftdpw", "random", "none")), fct_match(reward_type, "ex_post")) |&gt; </span></span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a><span class="st">  group_by(plan_type, step) |&gt; </span></span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a><span class="st">  summarize(mean_reward_diff = mean(reward_diff), .groups = "drop") |&gt;</span></span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a><span class="st">  pivot_wider(id_cols = step, names_from = plan_type, values_from = mean_reward_diff) |&gt; </span></span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a><span class="st">  pivot_longer(c(none, random), names_to = "baseline_policy", values_to = "baseline") |&gt; </span></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a><span class="st">  mutate(gain_per = (pftdpw - baseline) / baseline) |&gt;  </span></span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a><span class="st">  ggplot(aes(step)) +</span></span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a><span class="st">  geom_line(aes(y = gain_per, color = baseline_policy)) +</span></span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_x_continuous("Step", breaks = seq(maxstep)) +</span></span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_y_continuous("", labels = scales::label_percent()) +</span></span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_color_discrete("Compared to", labels = plan_labels) +</span></span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a><span class="st">  theme(panel.grid.minor.x = element_blank(), legend.position = "top", legend.direction = "vertical") +</span></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a><span class="st">  guides(linetype = "none") +</span></span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a><span class="st">  NULL</span></span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a><span class="ot">```</span></span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>To provide a clearer comparison, we calculate the percentage difference between the highest performing policies <span class="ot">and</span> two baseline policies: (i) the <span class="fu">no</span><span class="ot">-e</span>valuation policy, <span class="ot">and</span> (ii) the random Bayesian policy (which is roughly on par with the random frequentist policy). When compared to the policy of never re<span class="ot">-e</span>valuating a program once it is selected <span class="kw">for</span> implementation, we observe that the Bayesian evaluate<span class="ot">-s</span>econd<span class="ot">-b</span>est <span class="ot">and</span> the PFT-DPW offline policies show an average accumulated welfare that is more than <span class="dv">20</span> percent higher after four episode steps, <span class="ot">and</span> surpasses <span class="dv">30</span> percent after seven steps. In comparison to the frequentist policies (evaluate<span class="ot">-s</span>econd<span class="ot">-b</span>est <span class="ot">and</span> random), the highest performing policies show around <span class="dv">20</span> percent improvement after five steps.</span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a><span class="co"># Conclusion</span></span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a><span class="st">#| label: fig-step1-returns</span></span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-cap: "The distribution of utility gains at </span><span class="dt">$t</span><span class="st"> = 1</span><span class="wa">$,</span><span class="st"> comparing the hypothetical best policy, the frequentist policy, and the Bayesian policy."</span></span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-cap-location: bottom</span></span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-width: 3</span></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-height: 2</span></span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a><span class="st">#| column: margin </span></span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a><span class="st">vn_util_diff |&gt;</span></span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a><span class="st">  filter(step == 1, accum, discounted, fct_match(plan_type, c("best", "random", "freq")), fct_match(reward_type, "ex_post")) |&gt; </span></span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a><span class="st">  ggplot(aes(x = plan_type)) +</span></span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a><span class="st">  #tidybayes::stat_pointinterval(aes(x = reward_diff), point_interval = mean_qi, .width = 0.0) +</span></span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a><span class="st">  tidybayes::stat_dist_halfeye(aes(y = reward_diff), alpha = 0.25, .width = c(0.5, 0.8)) +</span></span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a><span class="st">  geom_vline(xintercept = 0, linetype = "dotted") +</span></span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_x_discrete(</span></span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a><span class="st">    "", </span></span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a><span class="st">    labels = c("best" = "Best", "freq" = "Frequentist", random = "Random")</span></span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a><span class="st">  ) +</span></span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_y_continuous("") +</span></span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a><span class="st">  #labs(title = "Accumulated Utility Improvement Compared to No Implementation", subtitle = "First Step Only") +</span></span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a><span class="st">  #facet_wrap(vars(reward_type), ncol = 1, scales = "free_y", labeller = as_labeller(c(ex_ante = "ex ante", ex_post = "ex post"))) +</span></span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a><span class="st">  coord_cartesian(ylim = c(-0.75, 0.75)) +</span></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a><span class="st">  NULL</span></span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a><span class="ot">```</span></span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a>In conclusion, through the construction of a simple simulacrum of the problem faced by an Effective Altruism funder <span class="ot">and</span> the consideration of planning policies over <span class="fu">time</span>, it is evident that there is a significant gap in accumulated welfare between the more naive versions of policies <span class="ot">and</span> the more probabilistic <span class="ot">and</span> sophisticated policies. This gap becomes more pronounced <span class="kw">when</span> we consider the sequential problem, as opposed to the one<span class="ot">-s</span>hot problem. For instance, in <span class="dt">@fig</span><span class="ot">-s</span>tep1<span class="ot">-r</span>eturns, we can see that there is little difference between a frequentist NHST policy <span class="ot">and</span> a probabilistic one in the first step. However, as more steps are taken, differences between the policies become apparent, underscoring the importance of considering the longer<span class="ot">-t</span>erm implications of planning policies.</span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a><span class="st">#| label: fig-returns-compare-include-best</span></span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-width: 3</span></span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-height: 4</span></span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a><span class="st">#| fig-cap: Mean accumulated discounted utility gains, compared to a no program implementation policy.</span></span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a><span class="st">#| column: margin</span></span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a><span class="st">vn_util_diff |&gt;</span></span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a><span class="st">  filter(accum, discounted, fct_match(plan_type, c("evalsecond", "best")), fct_match(reward_type, "ex_post")) |&gt; </span></span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a><span class="st">  ggplot(aes(step)) +</span></span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a><span class="st">  tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Mean"), .width = 0.0, linewidth = 0.25, point_interval = mean_qi) +</span></span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a><span class="st">  # tidybayes::stat_lineribbon(aes(y = reward_diff, fill = plan_type, color = plan_type, linetype = "Median"), .width = 0.0, linewidth = 0.25, point_interval = median_qi) +</span></span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_x_continuous("Step", breaks = seq(maxstep)) +</span></span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_y_continuous("Mean Accumulated Utility Gains", breaks = seq(0, 2, 0.2)) +</span></span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a><span class="st">  #scale_linetype_manual("", values = c("Mean" = "dashed", "Median" = "solid")) +</span></span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a><span class="st">  scale_color_discrete(</span></span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a><span class="st">    "Policy", </span></span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a><span class="st">    labels = plan_labels, </span></span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a><span class="st">    aesthetics = c("color", "fill")</span></span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a><span class="st">  ) +</span></span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a><span class="st">  # facet_wrap(vars(reward_type), ncol = 1, scales = "free_y", labeller = as_labeller(c(ex_ante = "ex ante", ex_post = "ex post"))) +</span></span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a><span class="st">  # labs(title = "Accumulated Utility Improvement Compared to No Implementation") +</span></span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a><span class="st">  theme(panel.grid.minor.x = element_blank(), legend.position = "top", legend.direction = "vertical") +</span></span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a><span class="st">  guides(linetype = "none") +</span></span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a><span class="st">  NULL</span></span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a><span class="ot">```</span></span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a>Even considering these factors, the gap between <span class="kw">our</span> best performing policy <span class="ot">and</span> the best possible hypothetical policy remains substantial (as seen in <span class="dt">@fig</span><span class="ot">-r</span>eturns<span class="ot">-c</span>ompare-include<span class="ot">-b</span>est). This suggests that there is likely more room <span class="kw">for</span> improvement in order to approach an optimal approach[^tuning]. </span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a>Here are some possibilities <span class="kw">for</span> enhancing this <span class="fu">study</span> to better reflect real<span class="ot">-w</span>orld environments <span class="ot">and</span> develop more effective policies (in <span class="fu">no</span> particular order):</span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Introduce varying program costs to consider the question of cost<span class="ot">-e</span>ffectiveness, as the cost of implementing different programs can have a significant impact on decision-making.</span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Explore how the concept of leverage could influence policy decisions, as certain programs may have a greater ability to leverage resources <span class="ot">and</span> create broader impact.</span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Allow <span class="kw">for</span> different population sizes in the simulation, as population size can affect the scalability <span class="ot">and</span> impact of interventions.</span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a> <span class="ot">*</span> Consider the potential <span class="kw">for</span> programs to target multiple populations with some correlation, <span class="ot">and</span> <span class="kw">for</span> populations to support multiple programs with potential complementarity <span class="ot">and</span> substitution effects, as this can reflect the complexity <span class="ot">and</span> interrelatedness of real<span class="ot">-w</span>orld scenarios.</span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Incorporate non<span class="ot">-s</span>tationarity into the hyperstates, effectively adding correlation between steps <span class="ot">and</span> potentially improving predictions <span class="ot">and</span> the need <span class="kw">for</span> re<span class="ot">-e</span>valuation, as real<span class="ot">-w</span>orld environments are dynamic <span class="ot">and</span> evolve over <span class="fu">time</span>.</span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Account <span class="kw">for</span> potential diminishing treatment effects over <span class="fu">time</span> as the control outcome moves closer to the treatment level, as this can affect the long<span class="ot">-t</span>erm effectiveness of interventions.</span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Consider the quality of programs <span class="ot">or</span> population compliance <span class="ot">and</span> how it may vary over <span class="fu">time</span>, as program quality <span class="ot">and</span> population behavior can impact outcomes in real<span class="ot">-w</span>orld scenarios.</span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Explore differences in evaluations <span class="kw">for</span> implemented programs versus non-implemented ones, as this can introduce potential scale effects <span class="ot">and</span> reflect the challenges of transitioning from proof<span class="ot">-o</span>f<span class="ot">-c</span>oncept studies to scaled programs.</span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Restrict the implementation action choices to prevent rapid changes between programs due to fixed costs, as it may <span class="ot">not</span> always be feasible to shut down <span class="ot">and</span> resume programs in quick succession. For example, disallow restarting a program once abandoned to reflect real<span class="ot">-w</span>orld constraints.</span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Allow <span class="kw">for</span> program entry <span class="ot">and</span> <span class="fu">exit</span> over <span class="fu">time</span> to capture the dynamic nature of program availability <span class="ot">and</span> effectiveness.</span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a><span class="ot">*</span> Analyze the sensitivity of the simulation to varying the environment<span class="ot">'</span><span class="ss">s hyperparameters, $\boldsymbol{\xi}$, to better understand the robustness of the results to different parameter settings.</span></span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a><span class="ss">* Consider offline policy calculation methods (e.g., deep reinforcement learning) to further optimize policy performance.</span></span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;!-- Moral uncertainty, ambguity, and moral weights --&gt;</span></span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a><span class="ss">These enhancements can help to make the simulation model more accurate and reflective of real-world complexities, and enable the development of more effective policies for decision-making in the context of Effective Altruism funding.</span></span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a><span class="ss">[^tuning]: It should be mentioned that in this experiment I did not attempt simulations very varying values of the prior hyperparameters, $\boldsymbol{\xi}$, or the PFT-DPW algorithm hyperparameters.</span></span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a><span class="ss">{{&lt; pagebreak &gt;}}</span></span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a><span class="ss">#| eval: false</span></span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a><span class="ss">test_data &lt;- julia_eval("test_data")</span></span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a><span class="ss">library(cmdstanr)</span></span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a><span class="ss">library(posterior)</span></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a><span class="ss">sim_model &lt;-cmdstan_model("../FundingPOMDPs.jl/stan/sim_model.stan")</span></span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a><span class="ss">test_stan_data &lt;- lst(</span></span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a><span class="ss">  fit = TRUE, sim = FALSE, sim_forward = FALSE,</span></span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a><span class="ss">  n_control_sim = 0,</span></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a><span class="ss">  n_treated_sim = 0,</span></span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a><span class="ss">  n_study = 1,</span></span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a><span class="ss">  study_size = 50,</span></span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a><span class="ss">  y_control = test_data |&gt; filter(!t) |&gt; pull(y),</span></span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a><span class="ss">  y_treated = test_data |&gt; filter(t) |&gt; pull(y),</span></span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a><span class="ss"> </span></span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a><span class="ss">  sigma_eta_inv_gamma_priors = TRUE, </span></span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a><span class="ss">  mu_sd = 1,</span></span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a><span class="ss">  tau_mean = 0,</span></span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a><span class="ss">  tau_sd = 0.5,</span></span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a><span class="ss">  sigma_sd = 0,</span></span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a><span class="ss">  eta_sd = c(0, 0, 0),</span></span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a><span class="ss">  sigma_alpha = 18.5,</span></span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a><span class="ss">  sigma_beta = 30,</span></span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a><span class="ss">  eta_alpha = 26.4,</span></span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a><span class="ss">  eta_beta = 20</span></span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a><span class="ss">)</span></span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a><span class="ss">fit &lt;- sim_model$sample(test_stan_data, parallel_chains = 4)</span></span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a><span class="ss">dr &lt;- as_draws_rvars(fit)</span></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a><span class="ss">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>